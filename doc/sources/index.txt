.. colibri documentation master file, created by
   sphinx-quickstart on Mon Oct  8 11:38:12 2012.
   You can adapt this file completely to your liking, but it should at least
   contain the root `toctree` directive.


****************************
Colibri Documentation 
****************************

.. toctree::
   :maxdepth: 3

Introduction
===================


Colibri Core is a set of tools as well as a C++ and Python library for working with basic linguistic constructions such as n-grams and skipgrams (i.e patterns with one or more gaps, either of fixed or dynamic size) in a quick and memory-efficient way. At the core is the tool ``colibri-patternmodeller`` which allows you to build, view, manipulate and query pattern models.

The Colibri software is developed in the scope of the Ph.D. research project **Constructions as Linguistic Bridges**. This research examines the identification and extraction of aligned constructions or patterns across natural languages, and the usage of such constructions in Machine Translation. The aligned constructions are not identified on the basis of an extensive and explicitly defined grammar or expert database of linguistic knowledge, but rather are implicitly distilled from large amounts of example data. Our notion of constructions is broad and transcends the idea of words or variable-length phrases.

This documentation will illustrate how to work with the various tools and the library of colibri, as well as elaborate on the implementation of certain key aspects of the software.

Installation
===============

Colibri is hosted on `github <http://github.com/proycon/colibri-core/>`_ and should be retrieved through the versioning control system ``git``. Provided git is installed on your system, this is done as follows::

	$ git clone http://github.com/proycon/colibri-core.git
	
You need to compile the software, if you want FoLiA support you will need to first to install the following dependency:

 * **libfolia**; obtainable from `the FoLiA website <http://proycon.github.com/folia>`_,  follow the instructions included with libfolia to install it.

In addition to the C/C++ compiler (``gcc``), the build process for colibri makes use of ``autoconf`` and ``automake``. Make sure these are installed on your system. Also install the package ``autoconf-archive`` if available on your distribution. Colibri can now be compiled and installed::

  $ cd colibri
  $ bash bootstrap
  $ ./configure [--with-libfolia=/path/to/libfolia]
  $ make
  $ make install
  
You can optionally pass a prefix if you want to install colibri in a different location::

  $ ./configure --prefix=/usr/local/
  
 
Keeping colibri up to date
-----------------------------

Colibri is always under heavy development. Update your colibri copy by issuing a git pull::

 $ git pull
 
And then recompile as per the above instructions.

General usage instructions
---------------------------------

Colibri consist of various programs, each of which will output an extensive overview of available parameters if the parameter ``-h`` is passed. Each program is designed for a specialised purpose, with specific input and output formats. It is often needed to call multiple programs in succession to obtain the final analysis or model you desire. 

Corpus Class Encoding
================================

Introduction
----------------------

Computation on large datasets begs for solutions to keep memory consumption
manageable. Colibri requires that input corpora are converted into a compressed
binary form. In this form each word-type in the corpus is represented by a
numeric class. Highly frequent word-types get assigned low class numbers and
less frequent word-types get higher class numbers. The class is represented in
a dynamic-width byte-array, rather than a fixed-width integer.Patterns are
encoded per word, each word starts with a size marker of one byte indicating the number of bytes
are used for that word. The specified number of bytes that follow encode the
word class. Instead of a size marker, byte values of 128 and above are reserved for special markers,
such as encoding gaps and structural data. Finally, the pattern as a whole is
ended by a null byte.

All internal computations of all tools in colibri proceed on this internal
representation rather than actual textual strings, keeping running time shorter
and memory footprint significantly smaller.

Class-encoding your corpus
-----------------------------------

When working with colibri, you first want to **class encode** your corpus. This is done by the program ``colibri-classencode``. It takes as input a *tokenised* monolingual corpus in plain text format, containing *one sentence per line*. Each line should be delimited by a single newline character (unix line endings). Colibri is completely agnostic when it comes to the character encoding of the input. Given a corpus file ``yourcorpus``, class encoding is done as follows::

	$ colibri-classencode yourcorpus

This results in two files:

 * ``yourcorpus.colibri.cls`` - This is the class file; it lists all word-types and class numbers.  
 * ``yourcorpus.colibri.dat`` - This is the corpus is encoded binary form. It is a lossless compression that is roughly half the size of the original  

If your corpus is not tokenised yet, you can consider using the tokeniser `ucto <http://ilk.uvt.nl/ucto>`_ (not part of colibri), this will also do sentence detection and output one line per sentence::

	$ ucto -L en -n untokenisedcorpus.txt > tokenisedcorpus.txt
	
The above sample is for English (``-L en``), several other languages are also supported.

In addition to this plain text input. The class encoder also supports *FoLiA XML* (`folia website <http://proycon.github.com/folia>`_) if you compiled with FoLiA support, make sure such files end with the extension ``xml`` and they will be automatically interpreted as FoLiA XML::

	$ colibri-classencode yourcorpus.xml
	

It is possible to encode multiple corpus files similtaneously, but generating a joined class file::

	$ colibri-classencode yourcorpus1 yourcorpus2
	
This results in ``yourcorpus1.colibri.cls`` and ``yourcorpus1.colibri.dat`` and ``yourcorpus2.colibri.dat``. The class file spans both despite the name. An explicit name can be passed with the ``-o`` flag. It is also possible to encode multiple corpora in a single unified file by passing the ``-u`` flag. This is often desired if you want to train a pattern model on all the joined data::

	$ colibri-classencode -o out -u yourcorpus1 yourcorpus2

This will produce ``out.colibri.dat`` and ``out.colibri.cls``.


If you want to specify a large number of files, you can use the ``-l`` flag and specify a filename from which all input filenames will be read, one per line.

Class-decoding your corpus
------------------------------

Given an encoded corpus and a class file, the original corpus can always be reconstructed. This we call *class decoding* and is done using the ``colibri-classdecode`` program::
   
 $ colibri-classdecode -f yourcorpus.colibri.dat -c yourcorpus.colibri.cls

Partial decoding can be done by specifying start and end line numbers using the
flags ``-s`` and ``-e`` respectively.

Output will be to ``stdout``, you can redirect it to a file as follows::

 $ colibri-classdecode -f yourcorpus.colibri.dat -c yourcorpus.colibri.cls > yourcorpus.txt

.. _classencodetraintest:
Class-encoding with existing classes
----------------------------------------

Sometimes you want to encode new data using the same classes already used for another data set. For instance when comparing corpora, it is vital that the same classes are used, i.e. that identical words are assigned identical numerical classes. This also applies when you are working with a training set and a separate test set, or are otherwise interested in a comparative analysis between two comparable datasets. The initial class file is built on the training set, and it can be reused to encode the test set:

You can encode a dataset, here named ``testset.txt`` using an existing class file, ``trainset.colibri.cls``, as follows::

   $ colibri-classencode -f testset.txt -c trainset.colibri.cls 

This will result in an encoded corpus ``testset.colibri.dat`` and an *extended* class file ``testset.colibri.cls``, which is a superset of the original ``trainset.cls``, adding only those classes that did not yet exist in the training data.


Pattern Modeller
===============================

Introduction
-----------------------

The ``colibri-patternmodeller`` program is used to create pattern models capturing recurring patterns from a monolingual corpus. The extracted patterns are n-grams or skip-grams, where a skip-gram is an n-gram with one or more gaps of either a predefined size, thus containing unspecified or wildcard tokens, or of dynamic width.

In the internal pattern representation, in the place of the size marker, byte
value 128 is used for a fixed gap of a single token, and can be repeated for
gaps of longer length, byte value 129 is used for a gap of unspecified dynamic
width. 

The pattern finding algorithm is iterative in nature and is guaranteed to find
all n-grams above a specified occurrence threshold and optionally given a
maximum size for n. It does so by iterating over the corpus n times, iterating
over all possible values for n in ascending order. At each iteration, a sliding
window extracts all n-grams in the corpus for the size is question. An n-gram
is counted in a hashmap data structure only if both n-1-grams it by definition
contains are found during the previous iteration with an occurrence above the
set threshold.  The exception are unigrams, which are all by definition counted
if they reach the threshold, as they are already atomic in nature. At the end
of each iteration, n-grams not making the occurrence threshold are
pruned. This simple iterative technique reduces the memory footprint compared
to the more naive approach of immediately storing all in a hashmap, as it
prevents the storing of lots of patterns not making the threshold by discarding
them at an earlier stage. 

At the beginning of each iteration of n, all possible ways in which any n-gram
of size n can contain gaps is computed. When an n-gram is found, various
skip-grams are tried in accordance with these gap configurations. This is
accomplished by 'punching holes' in the n-gram, resulting in a skip-gram. If
all consecutive parts of this skip-gram were counted during previous iterations
and thus made the threshold, then the skip-gram as a whole is counted,
otherwise it is discarded. After each iteration, pruning again takes places to
prune skip-grams that are not frequent enough.

The pattern finder can create either indexed or unindexed models. For indexed
models, the precise location of where an n-gram or skipgram instance was found
in the corpus is recorded. This comes at the cost of much higher memory usage,
but is necessary for more strongly constrained skip extraction, as well as for
extracting relations between patterns at a later stage. Indexed models by
default also maintain a reverse index allowing, and even unindexed models do so
during building.

Note that for fixed-size skipgrams in indexed models, the various fillings
for the gaps can be reconstructed precisely.

If you are only interested in simple n-gram or simple skip-gram counts, then an
unindexed model may suffice. 

Creating a pattern model
----------------------------

First make sure to have class-encoded your corpus. Given this encoded corpus, ``colibri-patternmodeller`` can be invoked to produce an indexed pattern model. The occurrence threshold is specified with parameter ``-t``, patterns occuring less will not be counted. The default value is two.  The maximum value for n, i.e. the maximum n-gram/skipgram size, can be restricted using the parameter ``-l``.:: 

	$ colibri-patternmodeller -f yourcorpus.dat -t 10 
	
This will result in a pattern model ``yourcorpus.indexedpatternmodel.colibri``. This model is stored in a binary format. To print it into a human readable presentation it needs to be decoded. The ``colibri-patternmodeller`` program can do this by specifying an input model using the ``-i`` flag, the class file using the ``-c`` parameter, and the desired action to print it all using ``-P``::

	$ colibri-patternmodeller -i yourcorpus.indexedpatternmodel.colibri -c yourcorpus.colibri.cls -P
	
Output will be to ``stdout`` in a tab delimited format, with the first line reserved for the header. This facilitates easy parsing as you can just load it into any software accepting CSV files, such as spreadsheets. An excerpt follows::

    PATTERN	COUNT	TOKENS	COVERAGE	CATEGORY	SIZE	FREQUENCY	REFERENCES
    For	2	2	0.0059	ngram	1	0.0121	11:0 15:0
    death	2	2	0.0059	ngram	1	0.0121	11:5 23:7
    who	2	2	0.0059	ngram	1	0.0121	15:1 21:5
    .	4	4	0.0118	ngram	1	0.0242	5:6 9:4 10:6 13:4
    be	4	4	0.0118	ngram	1	0.0242	1:1 1:5 9:2 35:3
    flee	2	2	0.0059	ngram	1	0.0121	36:1 36:5
    not to	4	8	0.0235	ngram	2	0.1538	1:3 36:3 37:3 38:3


The various columns are:

* **Pattern** - The actual pattern. Gaps in skipgrams are represented as ``{*x*}`` where x is a number representing the size of the skip. Variable-width skipgrams are just ``{*}``. 
* **Occurrence count** - The absolute number of times this pattern occurs
* **Tokens** - The absolute number of tokens in the corpus that this pattern covers. Computed as ``occurrencecount * n``. 
* **Coverage** - The number of covered tokens, as a fraction of the total number of tokens.
* **Category** - The type of pattern.
* **Size** - The length of the n-gram or skipgram in words/tokens.
* **Frequency** - The frequency of the pattern *within its category and
  size class*, so for an ngram of size two, the frequency indicates the
  frequency amongst all bigrams.
* **References** - A space-delimited list of indices in the corpus that correspond to a occurrence of this pattern. Indices are in the form ``sentence:token`` where sentence starts at one and token starts at zero. This column is only available for indexed models.
 
The pattern model created in the previous example did not yet include skip-grams, these have to be explicitly enabled with the ``-s`` flag. When this is used, another options becomes available for consideration:

* ``-T [value]`` - Only skipgrams that have at least this many different types
  as skip content, i.e. possible options filling the gaps, will be considered.
  The default is set to two.
  
Here is an example of generating an indexed pattern model including skipgrams::

	$ colibri-patternmodeller -f yourcorpus.colibri.dat -t 10 -s -T 3

If you want to generate unindexed models, simply add the flag ``-u``. Do note that for unindexed models the parameter ``-T`` has no effect, it will extract all skipgrams it can find as if ``-T`` were set to one! If you want decent skpigrams, you're best off with an indexed model. Note that indexed models can always be read and printed in an unindexed way (with the ``-u`` flag); but unindexed models can not be read in an indexed way, as they simply lack indices::

	$ colibri-patternmodeller -d yourcorpus.unindexedpatternmodel.colibri -c yourcorpus.cls -u 
	$ colibri-patternmodeller -d yourcorpus.indexedpatternmodel.colibri -c yourcorpus.cls -u 


Statistical reports and histograms
----------------------------------

If you have a pattern model, you can generate a statistical report which includes information on the number of occurrences and number of types for patterns, grouped for n-grams or skipgrams for a specific value of *n*. A report is generated using the ``-R`` flag, the input model is specified using ``-i``::

	   $ colibri-patternmodeller -i yourcorpus.indexedpatternmodel.colibri -R

Example output::

 REPORT
 ----------------------------------
                             PATTERNS    TOKENS  COVERAGE     TYPES
 Total:                             -       340         -       177
 Uncovered:                         -       175    0.5147       136
 Covered:                          69       165    0.4853        41
 
   CATEGORY N (SIZE)   PATTERNS    TOKENS  COVERAGE     TYPES OCCURRENCES
        all       all        69       165    0.4853        41         243
        all         1        40       165    0.4853        40         165
        all         2        11        26    0.0765        13          26
        all         3         7        17    0.0500         9          19
        all         4         5        10    0.0294         9          14
        all         5         5         9    0.0265         9          17
        all         6         1         2    0.0059         6           2
     n-gram       all        62       165    0.4853        40         215
     n-gram         1        40       165    0.4853        40         165
     n-gram         2        11        26    0.0765        13          26
     n-gram         3         5        12    0.0353         8          12
     n-gram         4         3         6    0.0176         6           6
     n-gram         5         2         4    0.0118         6           4
     n-gram         6         1         2    0.0059         6           2
   skipgram       all         7         7    0.0206         6          28
   skipgram         3         2         7    0.0206         4           7
   skipgram         4         2         4    0.0118         4           8
   skipgram         5         3         5    0.0147         5          13

Some explanation is in order to correctly interpret this data. There are three
columns:

    * **Pattern** - The number of distinct patterns
    * **Tokens** - The number of tokens that is covered. This is only available
      for indexed models, for unindexed models it is either omitted or the
      number shown is maximum projection.
    * **Coverage** - The number of tokens covered as a fraction of the total number of tokens. Only for indexed models.
    * **Types** - The number of unique **word** types covered
    * **Occurrences** - Cumulative occurrence count of all the patterns in
      the group. Used as a basis for computing frequency.

Pattern models store how many of the tokens and types in the original corpus were covered. Tokens and types not covered did not make the set thresholds. Make sure to use indexed models if you want accurate coverage data.

Similarly, a histogram can also be generated, using the ``-H`` flag::
	   
        $ colibri-patternmodeller -i yourcorpus.indexedpatternmodel.colibri -H

Example output::

        OCCURRENCES   PATTERNS
        2   39
        3   5
        4   13
        5   5
        6   1
        7   1
        8   1
        10  1
        13  1
        14  1
        15  1


Training and testing coverage
--------------------------------

An important quality of pattern models lies in the fact that pattern models can be compared. More specifically, you can train a pattern model on a corpus and test it on another corpus, which yields another pattern model containing only those patterns that occur in both training and test data. The difference in count, frequency and coverage can then be easily be compared.

Make sure to use the same class file for all datasets you are comparing. Instructions for this were given in :ref:`classencodetraintest`::
  
   $ patternfinder -f trainset.clsenc -t 10 -s -B -E
   $ patternfinder -d trainset.indexedpatternmodel.colibri -f testset.clsenc -t 10 -s -B -E
 
This results in a model ``testset.colibri.indexedpatternmodel``. This model can be used to generate a coverage report using the ``-C`` flag::

   $ patternfinder -d yourcorpus.indexedpatternmodel.colibri -C  





Query mode
--------------

The pattern modeller has query mode which allows you to quickly extract patterns from test sentences or fragments thereof. The query mode is invoked by loading a pattern model (``-i``), a class file (``-c``) and the ``-Q`` flag. The query mode can be run interactively as it takes input from ``stdin``, one *tokenised* sentence per line. The following example illustrates this, the sentence *"This is a test ."* was typed as input::[MC

	$ patternfinder -d europarl25k-en.indexedpatternmodel.colibri -c europarl25k-en.cls -Q                                                        
	Loading model
	Loading class decoder europarl25k-en.cls
	Loading class encoder europarl25k-en.cls
	Starting query mode:
	1>> This is a test .
	1:0	This	1085	0.001593135275538986
	1:0	This is	395	0.001159978679885529
	1:0	This is a	64	0.0002819188690354704
	1:0	This {*1*} a	66	0.0002907288336928288
	1:1	is	10570	0.0155202210713798
	1:1	is a	947	0.002781012176839484
	1:2	a	10272	0.01508265949339767
	1:2	a test	2	5.8733097715723e-06
	1:3	test	30	4.404982328679225e-05
	1:4	.	23775	0.03490948495478285

The output starts with an index in the format ``sentence:token``, the pattern found, and the next two values are the absolute occurrence count and the coverage ratio.

Graph Models
===================

Introduction
---------------------

A pattern model contains a wide variety of patterns; a graph model goes a step further by making explicit relationships between the various patterns in this model. These relationships can be visualised as a directed graph, in which the nodes represent the various patterns (n-grams and skipgrams), and the edges represent the relations. The following relations are distinguished; note that as the graph is directed relations often come in pairs; one relationship for each direction: 

* **Subsumption relations** - Patterns that are subsumed by larger patterns are called *children*, the larger patterns are called *parents*. These are the two subsumption relations that can be extracted from an indexed pattern model.
* **Successor relations**  - If a pattern A and a pattern B form part of a combined pattern A B, then the two patterns are in a successor/predecessor relationship.
* **Template relations** - Template relations indicate abstraction and go from n-grams to skipgrams. An example of a template relation is ``to be or not to be`` to ``to be {*1*} not {*1*} be``. The reverse direction is called the instance-relationship, as an specific n-gram is an instance of a more abstract template.
* **Skip content relations** - Relations between patterns that can be used to fill the gaps of higher patterns are called skip content relations. These can go in two directions; skipgram to skip content and skip content to skipgram. Example: ``to`` is a in a skip-content to skipgram relationship with  ``to be {*1*} not {*1*} be``.

In addition to the relations, a graph model can also compute a so-called *exclusivity count* and *exclusivity ratio* for each pattern. The exclusivity count of a pattern is the number of times the pattern occurs in the data *without* being subsumed by a larger found pattern. This exclusivity ratio is the exclusivity count as a fraction of the total occurrence count for the pattern. An exclusivity ratio of one indicates that the pattern is fully exclusive, meaning it is not subsumed by higher-order patterns. This notion of exclusivity may be of use in assessing compositionality of patterns. 


Computing a graph model
------------------------

The ``grapher`` program computes a graph model on the basis of an **indexed** pattern model created with ``patternfinder``. When computing a model, you need to explicitly specify which relations you desire to extract and include in your model. The more relations you include, the more memory will be required. To keep the models as small as possible, it is recommended to include only the relations you need. The following flags are available:

*	``-P`` - Compute/load subsumption relations from children to parents (reverse of -C)
*	``-C`` - Compute/load subsumption relations from parents to children (reverse of -P)
*	``-S`` - Compute/load skipgram to skipcontent relations
*	``-s`` - Compute/load skip-content to skipgram relations (reverse of -S)
*	``-L`` - Compute/load predecessor relations (constructions to the left)
*	``-R`` - Compute/load sucessor relations (constructions to the right)
*	``-T`` - Compute/load template relations
*	``-I`` - Compute/load instance relations (reverse of -T)
*	``-a`` - Compute/load all relations
*   ``-X`` - Compute/load count/ratios

The indexed pattern model that acts as input is specified using the ``-f`` flag. The following example generates a graph model with all relations::

	$ grapher -f yourcorpus.indexedpatternmodel.colibri -a
 
The graph model will be stored in binary form, in the file ``yourcorpus.graphpatternmodel.colibri``. 

Viewing and querying a graph model
------------------------------------

To decode this binary graph model into human readable form, read it in using the ``-d`` flag and pass a class file. In addition, you again need to pass what relations you want to load, as it is also possible to only load a subset of the relations. Simply use the ``-a`` flag if you want to load and output relations existing in the model::

  $ grapher -d yourcorpus.graphpatternmodel.colibri -c yourcorpus.cls -a

This will result in output to ``stdout`` in a tab-separated format, as illustrated below::

	#N      VALUE   OCC.COUNT       TOKENS  COVERAGE        XCOUNT  XRATIO  PARENTS CHILDREN        TEMPLATES       INSTANCES       SKIPUSAGE       SKIPCONTENT     SUCCESSORS      PREDECESSORS
	4       the summer break the    2       8       1.17466195431446e-05    0       0       1       8       1       0       0       0       0       1       
	2       that sustainable        2       4       5.8733097715723e-06     0       0       1       2       0       0       0       0       0       0       
	2       Kosovo ,        20      40      5.8733097715723e-05     4       0.2     5       2       0       0       1       0       0       1       
	2       which refer     2       4       5.8733097715723e-06     0       0       2       2       0       0       0       0       1       0       
	2       relief ,        2       4       5.8733097715723e-06     2       1       0       2       0       0       0       0       0       0       
	6       rule of law and respect for     3       18      2.642989397207535e-05   1       0.3333333333333333      2       20      0       0       1       0       0       2  

In the above example, only the number of relations for each type is shown, if you want to view the actual relations, you need to instruct ``grapher`` to output the the whole graph by adding the ``-g`` flag::


  $ grapher -d yourcorpus.graphpatternmodel.colibri -c yourcorpus.cls -a -g


	#N      VALUE   OCC.COUNT       TOKENS  COVERAGE        XCOUNT  XRATIO  PARENTS CHILDREN        TEMPLATES       INSTANCES       SKIPUSAGE       SKIPCONTENT     SUCCESSORS      PREDECESSORS
	4       the summer break the    2       8       1.17466195431446e-05    0       0       1       8       1       0       0       0       0       1       
	Parent relations - 1
		    before the summer break the     2       10      1.468327442893075e-05   2       1
	Child relations - 8
		    the     44027   44027   0.06464605232825341     90      0.002044200149908011
		    the summer      15      30      4.404982328679225e-05   5       0.3333333333333333
		    the summer break        2       6       8.80996465735845e-06    0       0
		    summer  30      30      4.404982328679225e-05   3       0.1
		    break   17      17      2.496156652918227e-05   2       0.1176470588235294
		    break the       5       10      1.468327442893075e-05   1       0.2
		    summer break the        2       6       8.80996465735845e-06    0       0
		    summer break    2       4       5.8733097715723e-06     0       0
	Predecessor relations - 1
		    before  357     357     0.0005241928971128277   36      0.1008403361344538
	Templates - 1
		    the {*2*} the   3266    13064   0.01918222971395513     3266    1

Outputting the whole graph may however produce a lot of unwanted output. Often you want to query your graph model for only one pattern. This is done with the ``-q`` parameter. In the following example we query our model for the pattern "summer"::

	$ grapher -d yourcorpus.graphpatternmodel.colibri -c yourcorpus.cls -a -q "summer"

	Outputting graph for "summer"
	Query:
		summer	30	30	4.40498e-05	3	0.1
	Parent relations - 16
		summer break	2	4	5.87331e-06	0	0
		after the summer	2	6	8.80996e-06	2	1
		the summer break the	2	8	1.17466e-05	0	0
		this summer .	5	15	2.20249e-05	5	1
		before the summer break	2	8	1.17466e-05	0	0
		of this summer	2	6	8.80996e-06	2	1
		the summer .	5	15	2.20249e-05	5	1
		before the summer	5	15	2.20249e-05	3	0.6
		the summer	15	30	4.40498e-05	5	0.333333
		before the summer break the	2	10	1.46833e-05	2	1
		this summer	11	22	3.23032e-05	2	0.181818
		the summer break	2	6	8.80996e-06	0	0
		this summer ,	4	12	1.76199e-05	4	1
		summer .	10	20	2.93665e-05	0	0
		summer ,	5	10	1.46833e-05	1	0.2
		summer break the	2	6	8.80996e-06	0	0
	Successor relations - 1
		break the	5	10	1.46833e-05	1	0.2
	Skipusage - 3
		this {*1*} ,	585	1755	0.00257691	585	1
		this {*1*} .	537	1611	0.00236548	537	1
		the {*1*} .	1496	4488	0.00658985	1496	1

It is possible to actually visualise the graph, ``grapher`` can output to the so-called dot-format used by the open-source graph visualisation software `graphviz <http://www.graphviz.org>`_. You can output to this format by specifying the ``-G`` flag. This works both with and without ``-q``, but if you do not specify a query the graph may turn out to be too huge to visualise::

	$ grapher -d yourcorpus.graphpatternmodel.colibri -c yourcorpus.cls -a -q "summer" -G > summer.dot
 
Graphviz will do the actual conversion to an image file, such as png::
 
	$ dot -Tpng summer.dot -o summer.png
 
This generated the following image:

.. image:: summer.png

.. TODO: Coverage report

