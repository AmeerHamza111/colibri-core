<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
  "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">


<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    
    <title>Colibri Documentation &mdash; Colibri Core 0.5.3 documentation</title>
    
    <link rel="stylesheet" href="_static/default.css" type="text/css" />
    <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
    
    <script type="text/javascript">
      var DOCUMENTATION_OPTIONS = {
        URL_ROOT:    './',
        VERSION:     '0.5.3',
        COLLAPSE_INDEX: false,
        FILE_SUFFIX: '.html',
        HAS_SOURCE:  true
      };
    </script>
    <script type="text/javascript" src="_static/jquery.js"></script>
    <script type="text/javascript" src="_static/underscore.js"></script>
    <script type="text/javascript" src="_static/doctools.js"></script>
    <link rel="top" title="Colibri Core 0.5.3 documentation" href="#" /> 
  </head>
  <body>
    <div class="related">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="genindex.html" title="General Index"
             accesskey="I">index</a></li>
        <li><a href="#">Colibri Core 0.5.3 documentation</a> &raquo;</li> 
      </ul>
    </div>  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body">
            
  <div class="section" id="colibri-documentation">
<h1>Colibri Documentation<a class="headerlink" href="#colibri-documentation" title="Permalink to this headline">¶</a></h1>
<div class="toctree-wrapper compound">
<ul class="simple">
</ul>
</div>
<div class="section" id="introduction">
<h2>Introduction<a class="headerlink" href="#introduction" title="Permalink to this headline">¶</a></h2>
<p>Colibri Core is software, consisting of command line tools as well as programming libraries. to quickly and efficiently count and extract patterns from large corpus data, to extract various statistics on the extracted patterns, and to compute relations between the extracted patterns. The employed notion of pattern or construction encompasses the following categories:</p>
<blockquote>
<div><ul class="simple">
<li><strong>n-gram</strong> &#8211; n consecutive words</li>
<li><strong>skipgram</strong> &#8211; An abstract pattern of predetermined length with one or multiple gaps (of specific size).</li>
<li><strong>flexgram</strong> &#8211; An abstract pattern without predetermined length, with one or more gaps.</li>
</ul>
</div></blockquote>
<p>N-gram extraction may seem fairly trivial at first, with a few lines in your favourite scripting language, you can move a simple sliding window of size <em>n</em> over your corpus and store the results in some kind of hashmap. This trivial approach however makes an unnecessarily high demand on memory resources, this often becomes prohibitive if unleashed on large corpora. Colibri Core tries to minimise these space requirements in several ways:</p>
<blockquote>
<div><ul class="simple">
<li><strong>Binary representation</strong> &#8211; Each word type is assigned a numeric class, which is encoded in a compact binary format in which highly frequent classes take less space than less frequent classes. Colibri core always uses this representation rather than a full string representation, both on disk and in memory.</li>
<li><strong>Informed counting</strong> &#8211; Counting is performed more intelligently by iteratively processing the corpus in several passes and quickly discarding patterns that won&#8217;t reach the desired occurrence threshold.</li>
</ul>
</div></blockquote>
<p>Skipgram and flexgram extraction are computationally more demanding but have been implemented with similar optimisations. Skipgrams are computed by abstracting over n-grams, and flexgrams in turn are computed either by abstracting over skipgrams, or directly from n-grams on the basis of co-occurrence information (mutual pointwise information). When patterns have been extracted, along with their counts and or index references to original corpus data, they form a so-called <em>pattern model</em>.</p>
<p>At the heart of Colibri Core lies the tool <tt class="docutils literal"><span class="pre">colibri-patternmodeller</span></tt> which allows you to build, view, manipulate and query pattern models.</p>
<p>The Colibri software is developed in the scope of the Ph.D. research project <strong>Constructions as Linguistic Bridges</strong>. This research examines the identification and extraction of aligned constructions or patterns across natural languages, and the usage of such constructions in Machine Translation. The aligned constructions are not identified on the basis of an extensive and explicitly defined grammar or expert database of linguistic knowledge, but rather are implicitly distilled from large amounts of example data. Our notion of constructions is broad and transcends the idea of words or variable-length phrases.</p>
<p>This documentation will illustrate how to work with the various tools and the library of colibri, as well as elaborate on the implementation of certain key aspects of the software.</p>
</div>
<div class="section" id="installation">
<h2>Installation<a class="headerlink" href="#installation" title="Permalink to this headline">¶</a></h2>
<p>Colibri is hosted on <a class="reference external" href="http://github.com/proycon/colibri-core/">github</a> and should be retrieved through the versioning control system <tt class="docutils literal"><span class="pre">git</span></tt>. Provided git is installed on your system, this is done as follows:</p>
<div class="highlight-python"><div class="highlight"><pre>$ git clone http://github.com/proycon/colibri-core.git
</pre></div>
</div>
<p>You need to compile the software, if you want FoLiA support you will need to first to install the following dependency. By default FoLiA support is disabled.</p>
<blockquote>
<div><ul class="simple">
<li><strong>libfolia</strong>; obtainable from <a class="reference external" href="http://proycon.github.com/folia">the FoLiA website</a>,  follow the instructions included with libfolia to install it.</li>
</ul>
</div></blockquote>
<p>In addition to the C/C++ compiler (<tt class="docutils literal"><span class="pre">gcc</span></tt>), the build process for colibri makes use of <tt class="docutils literal"><span class="pre">autoconf</span></tt> and <tt class="docutils literal"><span class="pre">automake</span></tt>. Make sure these are installed on your system. Also install the package <tt class="docutils literal"><span class="pre">autoconf-archive</span></tt> if available on your distribution. Colibri can now be compiled and installed:</p>
<div class="highlight-python"><div class="highlight"><pre>$ cd colibri
$ bash bootstrap
$ ./configure [--with-folia --with-folia-path=/path/to/libfolia]
$ make
$ make install
</pre></div>
</div>
<p>You can optionally pass a prefix if you want to install colibri in a different location:</p>
<div class="highlight-python"><div class="highlight"><pre>$ ./configure --prefix=/usr/local/
</pre></div>
</div>
<p>To install the Colibri Core Python library, you require Python 3 or higher with Cython 0.19 or above. On Debian/Ubuntu systems you will find these in the packages
<tt class="docutils literal"><span class="pre">python3</span></tt> and <tt class="docutils literal"><span class="pre">cython3</span></tt>. To compile and install the Python library, issue
the following command:</p>
<div class="highlight-python"><div class="highlight"><pre>$ python3 ./setup.py install
</pre></div>
</div>
<p>If you want to install in a customised non-global location, use <tt class="docutils literal"><span class="pre">--prefix</span></tt>
along with <tt class="docutils literal"><span class="pre">--include-dirs</span></tt> and <tt class="docutils literal"><span class="pre">--library-dirs</span></tt> to point to where the
C++ headers and the library are installed:</p>
<div class="highlight-python"><div class="highlight"><pre>$ python3 ./setup.py build_ext --include-dirs=/path/to/include/colibri-core  --library-dirs=/path/to/lib/  install --prefix=/path/to/somewhere/
</pre></div>
</div>
<p>If you plan to use Colibri-Core from Python with virtualenv or a distribution
such as Anaconda, we recommend to install all of colibri-core in this specific
location. To do this, you need need to call <tt class="docutils literal"><span class="pre">configure</span></tt> with <tt class="docutils literal"><span class="pre">--prefix</span></tt>, where the prefix is the directory where your
virtual environment, or Anaconda resides. The <tt class="docutils literal"><span class="pre">--prefix</span></tt> for <tt class="docutils literal"><span class="pre">setup.py</span></tt> is
the same, and the include-dirs and library-dirs should point to respectively
to the <tt class="docutils literal"><span class="pre">include/colibri-core</span></tt> and <tt class="docutils literal"><span class="pre">lib</span></tt> directories within this directory.</p>
<div class="section" id="keeping-colibri-up-to-date">
<h3>Keeping colibri up to date<a class="headerlink" href="#keeping-colibri-up-to-date" title="Permalink to this headline">¶</a></h3>
<p>Colibri is always under heavy development. Update your colibri copy by issuing a git pull:</p>
<div class="highlight-python"><div class="highlight"><pre>$ git pull
</pre></div>
</div>
<p>And then recompile as per the above instructions.</p>
</div>
<div class="section" id="general-usage-instructions">
<h3>General usage instructions<a class="headerlink" href="#general-usage-instructions" title="Permalink to this headline">¶</a></h3>
<p>Colibri consist of various programs and scripts, each of which will output an extensive overview of available parameters if the parameter <tt class="docutils literal"><span class="pre">-h</span></tt> is passed. Each program is designed for a specialised purpose, with specific input and output formats. It is often needed to call multiple programs in succession to obtain the final analysis or model you desire.</p>
</div>
</div>
<div class="section" id="quick-start-high-level-scripts">
<h2>Quick start: High-level scripts<a class="headerlink" href="#quick-start-high-level-scripts" title="Permalink to this headline">¶</a></h2>
<div class="section" id="id1">
<h3>Introduction<a class="headerlink" href="#id1" title="Permalink to this headline">¶</a></h3>
<p>Colibri Core comes with a set of scripts that provide simpler access to the
underlying tools and can be used from the command line by end-users to get
quick results. Input to these tools is always one or more plain text files, in
tokenised form, with one sentence per line.</p>
</div>
<div class="section" id="tokenisation">
<h3>Tokenisation<a class="headerlink" href="#tokenisation" title="Permalink to this headline">¶</a></h3>
<p>If your corpus is not tokenised yet, you can consider using the tokeniser <a class="reference external" href="http://ilk.uvt.nl/ucto">ucto</a> , which is not part of Colibri Core, Debian/Ubuntu
users may it in the repository (<tt class="docutils literal"><span class="pre">sudo</span> <span class="pre">apt-get</span> <span class="pre">install</span> <span class="pre">ucto</span></tt>), Mac OS X users
can find it in homebrew (<tt class="docutils literal"><span class="pre">brew</span> <span class="pre">install</span> <span class="pre">naiaden/lama/ucto</span></tt>). This will also do
sentence detection and, with the <tt class="docutils literal"><span class="pre">-n</span></tt> flag output one line per sentence, as
Colibri prefers:</p>
<div class="highlight-python"><div class="highlight"><pre>$ ucto -L en -n untokenisedcorpus.txt &gt; tokenisedcorpus.txt
</pre></div>
</div>
<p>The <tt class="docutils literal"><span class="pre">-L</span></tt> specifies the language of your corpus (English in this case), several others are
available as well.</p>
<p>Of course, you can use any other tokeniser of your choice.</p>
</div>
<div class="section" id="scripts">
<h3>Scripts<a class="headerlink" href="#scripts" title="Permalink to this headline">¶</a></h3>
<p>In addition to the core tools, described in the remainder of this
documentation, Colibri Core offers the following scripts:</p>
<blockquote>
<div><ul>
<li><p class="first"><tt class="docutils literal"><span class="pre">colibri-ngrams</span></tt> - Extracts n-grams of a particular size from the corpus
text, in the order they occur, i.e. by moving a sliding window over the text.</p>
</li>
<li><dl class="first docutils">
<dt><tt class="docutils literal"><span class="pre">colibri-freqlist</span></tt> - Extracts all n-grams from one or more corpus text files and</dt>
<dd><p class="first last">outputs a frequency list. Also allows for the extraction of skipgrams. By
default all n-grams are extract, but an occurrence threshold can be set with the <tt class="docutils literal"><span class="pre">-t</span></tt> flag.</p>
</dd>
</dl>
</li>
<li><dl class="first docutils">
<dt><tt class="docutils literal"><span class="pre">colibri-ngramstats</span></tt> - Prints a summary report on the ngrams in one or</dt>
<dd><p class="first last">more corpus text files. To get the full details on interpreting the output report,
read the section <em>Statistical Reports and Histograms</em>.</p>
</dd>
</dl>
</li>
<li><p class="first"><tt class="docutils literal"><span class="pre">colibri-histogram</span></tt> - Prints a histogram of ngram/skipgram occurrence count</p>
</li>
<li><p class="first"><tt class="docutils literal"><span class="pre">colibri-queryngrams</span></tt> - Interactive tool allowing you to query ngrams
from standard input, various statistics and relations can be outputted.</p>
</li>
<li><p class="first"><tt class="docutils literal"><span class="pre">colibri-reverseindex</span></tt> - Computes and prints a reverse index for the
specified corpus text file. For each token position in the corpus, it will
output what patterns are found there (i.e start at that very same position)</p>
</li>
<li><p class="first"><tt class="docutils literal"><span class="pre">colibri-loglikelihood</span></tt> - Computes the log-likelihood between patterns in
two or more corpus text files, which allows users to determine what words or
patterns are significantly more frequent in one corpus than the other.</p>
</li>
<li><p class="first"><tt class="docutils literal"><span class="pre">colibri-coverage</span></tt> - Computes overlap between a training corpus and a test
corpus, produces coverage metrics.</p>
</li>
</ul>
</div></blockquote>
<p>Users have to be aware, however, that these script only expose a
limited amount of the functionality of Colibri Core.</p>
</div>
</div>
<div class="section" id="corpus-class-encoding">
<h2>Corpus Class Encoding<a class="headerlink" href="#corpus-class-encoding" title="Permalink to this headline">¶</a></h2>
<div class="section" id="id2">
<h3>Introduction<a class="headerlink" href="#id2" title="Permalink to this headline">¶</a></h3>
<p>Computation on large datasets begs for solutions to keep memory consumption
manageable. Colibri requires that input corpora are converted into a compressed
binary form. The vocabulary of the corpus is converted to integer form, i.e.
each word-type in the corpus is represented by a numeric class. Highly frequent
word-types get assigned low class numbers and less frequent word-types get
higher class numbers. The class is represented in a dynamic-width byte-array,
rather than a fixed-width integer. Patterns are encoded per word, each word
starts with a size marker of one byte indicating the number of bytes are used
for that word. The specified number of bytes that follow encode the word class.
Instead of a size marker, byte values of 128 and above are reserved for special
markers, such as encoding gaps and structural data. Finally, the pattern as a
whole is ended by a null byte.</p>
<p>All internal computations of all tools in colibri proceed on this internal
representation rather than actual textual strings, keeping running time shorter
and memory footprint significantly smaller.</p>
</div>
<div class="section" id="class-encoding-your-corpus">
<h3>Class-encoding your corpus<a class="headerlink" href="#class-encoding-your-corpus" title="Permalink to this headline">¶</a></h3>
<p>When working with colibri, you first want to <strong>class encode</strong> your corpus. This is done by the program <tt class="docutils literal"><span class="pre">colibri-classencode</span></tt>. It takes as input a <em>tokenised</em> monolingual corpus in plain text format, containing <em>one sentence per line</em>, as a line is the only structural unit Colibri works with, extracted patterns will never cross line boundaries. Each line should be delimited by a single newline character (unix line endings). If you desire another structural unit (such as for example a tweet, or a paragraph), simply make sure each is on one line.</p>
<p>Colibri is completely agnostic when it comes to the character encoding of the input. Given a corpus file <tt class="docutils literal"><span class="pre">yourcorpus</span></tt>, class encoding is done as follows:</p>
<div class="highlight-python"><div class="highlight"><pre>$ colibri-classencode yourcorpus
</pre></div>
</div>
<p>This results in two files:</p>
<blockquote>
<div><ul class="simple">
<li><tt class="docutils literal"><span class="pre">yourcorpus.colibri.cls</span></tt> - This is the class file; it lists all word-types and class numbers.</li>
<li><tt class="docutils literal"><span class="pre">yourcorpus.colibri.dat</span></tt> - This is the corpus is encoded binary form. It is a lossless compression that is roughly half the size of the original</li>
</ul>
</div></blockquote>
<p>If your corpus is not tokenised yet, you can consider using the tokeniser <a class="reference external" href="http://ilk.uvt.nl/ucto">ucto</a> (not part of colibri), this will also do sentence detection and output one line per sentence:</p>
<div class="highlight-python"><div class="highlight"><pre>$ ucto -L en -n untokenisedcorpus.txt &gt; tokenisedcorpus.txt
</pre></div>
</div>
<p>The above sample is for English (<tt class="docutils literal"><span class="pre">-L</span> <span class="pre">en</span></tt>), several other languages are also supported.</p>
<p>In addition to this plain text input. The class encoder also supports <em>FoLiA XML</em> (<a class="reference external" href="http://proycon.github.com/folia">folia website</a>) if you compiled with FoLiA support, make sure such files end with the extension <tt class="docutils literal"><span class="pre">xml</span></tt> and they will be automatically interpreted as FoLiA XML:</p>
<div class="highlight-python"><div class="highlight"><pre>$ colibri-classencode yourcorpus.xml
</pre></div>
</div>
<p>The class file is the vocabulary of your corpus, it simply maps word strings to integer. You must always ensure that whenever you are working with multiple models, and you want to compare them, to use the exact same class file. It is possible to encode multiple corpus files similtaneously,  generating a joined class file:</p>
<div class="highlight-python"><div class="highlight"><pre>$ colibri-classencode yourcorpus1.txt yourcorpus2.txt
</pre></div>
</div>
<p>This results in <tt class="docutils literal"><span class="pre">yourcorpus1.colibri.cls</span></tt> and <tt class="docutils literal"><span class="pre">yourcorpus1.colibri.dat</span></tt> and <tt class="docutils literal"><span class="pre">yourcorpus2.colibri.dat</span></tt>. The class file spans both despite the name. An explicit name can be passed with the <tt class="docutils literal"><span class="pre">-o</span></tt> flag. It is also possible to encode multiple corpora in a single unified file by passing the <tt class="docutils literal"><span class="pre">-u</span></tt> flag. This is often desired if you want to train a pattern model on all the joined data:</p>
<div class="highlight-python"><div class="highlight"><pre>$ colibri-classencode -o out -u yourcorpus1.txt yourcorpus2.txt
</pre></div>
</div>
<p>This will produce <tt class="docutils literal"><span class="pre">out.colibri.dat</span></tt> and <tt class="docutils literal"><span class="pre">out.colibri.cls</span></tt>. You can use the <tt class="docutils literal"><span class="pre">-l</span></tt> option to read input filenames from file instead of command line arguments (one filename per line).</p>
<p>If you have a pre-existing class file you can load it with the <tt class="docutils literal"><span class="pre">-c</span></tt> flag, and use it to encode new data:</p>
<div class="highlight-python"><div class="highlight"><pre>$ colibri-classencode -c yourcorpus1.colibri.cls yourcorpus2.txt
</pre></div>
</div>
<p>This will produce a <tt class="docutils literal"><span class="pre">yourcorpus2.colibri.dat</span></tt>, provided that all of the word types already existed in <tt class="docutils literal"><span class="pre">yourcorpus1.colibri.cls</span></tt> (which usually is not the case, in which case an error will be shown.
To circumvent this error you have to specify how to deal with unknown words. There are two ways; the <tt class="docutils literal"><span class="pre">-U</span></tt> flag will encode all unknown word as a single word class dedicated to the task, whereas the <tt class="docutils literal"><span class="pre">-e</span></tt> flag will <em>extend</em> the specified class file with any new classes found. It has to be noted that this extension method spoils the optimal compression as classes are no longer strictly sorted by frequency. If you can all needed data in one go, then that is always preferred.</p>
<p>This setup, however, is often seen in a train/test paradigm:</p>
<div class="highlight-python"><div class="highlight"><pre>$ colibri-classencode -f testset.txt -c trainset.colibri.cls -e
</pre></div>
</div>
<p>This will result in an encoded corpus <tt class="docutils literal"><span class="pre">testset.colibri.dat</span></tt> and an <em>extended</em> class file <tt class="docutils literal"><span class="pre">testset.colibri.cls</span></tt>, which is a superset of the original <tt class="docutils literal"><span class="pre">trainset.cls</span></tt>, adding only those classes that did not yet exist in the training data.</p>
</div>
<div class="section" id="class-decoding-your-corpus">
<h3>Class-decoding your corpus<a class="headerlink" href="#class-decoding-your-corpus" title="Permalink to this headline">¶</a></h3>
<p>Given an encoded corpus and a class file, the original corpus can always be reconstructed (unless the <tt class="docutils literal"><span class="pre">-U</span></tt> option was used in encoding to allow unknown words). This we call <em>class decoding</em> and is done using the <tt class="docutils literal"><span class="pre">colibri-classdecode</span></tt> program:</p>
<div class="highlight-python"><div class="highlight"><pre>$ colibri-classdecode -f yourcorpus.colibri.dat -c yourcorpus.colibri.cls
</pre></div>
</div>
<p>Partial decoding can be done by specifying start and end line numbers using the
flags <tt class="docutils literal"><span class="pre">-s</span></tt> and <tt class="docutils literal"><span class="pre">-e</span></tt> respectively.</p>
<p>Output will be to <tt class="docutils literal"><span class="pre">stdout</span></tt>, you can redirect it to a file as follows:</p>
<div class="highlight-python"><div class="highlight"><pre>$ colibri-classdecode -f yourcorpus.colibri.dat -c yourcorpus.colibri.cls &gt; yourcorpus.txt
</pre></div>
</div>
</div>
</div>
<div class="section" id="pattern-modeller">
<h2>Pattern Modeller<a class="headerlink" href="#pattern-modeller" title="Permalink to this headline">¶</a></h2>
<div class="section" id="id4">
<h3>Introduction<a class="headerlink" href="#id4" title="Permalink to this headline">¶</a></h3>
<p>The <tt class="docutils literal"><span class="pre">colibri-patternmodeller</span></tt> program is used to create pattern models capturing recurring patterns from a monolingual corpus. The extracted patterns are n-grams or skip-grams, where a skip-gram is an n-gram with one or more gaps of either a predefined size, thus containing unspecified or wildcard tokens, or of dynamic width.</p>
<p>In the internal pattern representation, in the place of the size marker, byte
value 128 is used for a fixed gap of a single token, and can be repeated for
gaps of longer length, byte value 129 is used for a gap of unspecified dynamic
width.</p>
<p>The pattern finding algorithm is iterative in nature and is guaranteed to find
all n-grams above a specified occurrence threshold and optionally given a
maximum size for n. It does so by iterating over the corpus n times, iterating
over all possible values for n in ascending order. At each iteration, a sliding
window extracts all n-grams in the corpus for the size is question. An n-gram
is counted in a hashmap data structure only if both n-1-grams it by definition
contains are found during the previous iteration with an occurrence above the
set threshold.  The exception are unigrams, which are all by definition counted
if they reach the threshold, as they are already atomic in nature. At the end
of each iteration, n-grams not making the occurrence threshold are
pruned. This simple iterative technique reduces the memory footprint compared
to the more naive approach of immediately storing all in a hashmap, as it
prevents the storing of lots of patterns not making the threshold by discarding
them at an earlier stage.</p>
<p>At the beginning of each iteration of n, all possible ways in which any n-gram
of size <em>n</em> can contain gaps is computed. When an n-gram is found, various
skip-grams are tried in accordance with these gap configurations. This is
accomplished by &#8216;punching holes&#8217; in the n-gram, resulting in a skip-gram. If
all consecutive parts of this skip-gram were counted during previous iterations
and thus made the threshold, then the skip-gram as a whole is counted,
otherwise it is discarded. After each iteration, pruning again takes places to
prune skip-grams that are not frequent enough.</p>
<p>The pattern finder can create either indexed or unindexed models. For indexed
models, the precise location of where an n-gram or skipgram instance was found
in the corpus is recorded. This comes at the cost of much higher memory usage,
but is necessary for more strongly constrained skip extraction, as well as for
extracting relations between patterns at a later stage. Indexed models by
default also maintain a reverse index allowing, and even unindexed models do so
during building.</p>
<p>Note that for fixed-size skipgrams in indexed models, the various fillings
for the gaps can be reconstructed precisely.</p>
<p>If you are only interested in simple n-gram or simple skip-gram counts, then an
unindexed model may suffice.</p>
</div>
<div class="section" id="creating-a-pattern-model">
<h3>Creating a pattern model<a class="headerlink" href="#creating-a-pattern-model" title="Permalink to this headline">¶</a></h3>
<p>First make sure to have class-encoded your corpus. Given this encoded corpus,
<tt class="docutils literal"><span class="pre">colibri-patternmodeller</span></tt> can be invoked to produce an indexed pattern model.
Always specify the output file using the <tt class="docutils literal"><span class="pre">-o</span></tt> flag. The occurrence threshold
is specified with parameter <tt class="docutils literal"><span class="pre">-t</span></tt>, patterns occuring less will not be counted.
The default value is two.  The maximum value for n, i.e. the maximum
n-gram/skipgram size, can be restricted using the parameter <tt class="docutils literal"><span class="pre">-l</span></tt>.:</p>
<div class="highlight-python"><div class="highlight"><pre>$ colibri-patternmodeller -f yourcorpus.dat -t 10 -o yourcorpus.colibri.indexedpatternmodel
</pre></div>
</div>
<p>This outputted model <tt class="docutils literal"><span class="pre">yourcorpus.colibri.indexedpatternmodel</span></tt> is stored in a
binary format. To print it into a human readable presentation it needs to be
decoded. The <tt class="docutils literal"><span class="pre">colibri-patternmodeller</span></tt> program can do this by specifying an
input model using the <tt class="docutils literal"><span class="pre">-i</span></tt> flag, the class file using the <tt class="docutils literal"><span class="pre">-c</span></tt> parameter,
and the desired action to print it all using <tt class="docutils literal"><span class="pre">-P</span></tt>:</p>
<div class="highlight-python"><div class="highlight"><pre>$ colibri-patternmodeller -i yourcorpus.colibri.indexedpatternmodel -c yourcorpus.colibri.cls -P
</pre></div>
</div>
<p>Optionally, instead of or in addition to outputting a model to file using
<tt class="docutils literal"><span class="pre">-o</span></tt>, you can also print it directly with <tt class="docutils literal"><span class="pre">-P</span></tt>.</p>
<p>Output will be to <tt class="docutils literal"><span class="pre">stdout</span></tt> in a tab delimited format, with the first line
reserved for the header. This facilitates easy parsing as you can just load it
into any software accepting CSV files, such as spreadsheets. An excerpt
follows:</p>
<div class="highlight-python"><div class="highlight"><pre>PATTERN     COUNT   TOKENS  COVERAGE        CATEGORY        SIZE    FREQUENCY       REFERENCES
For 2       2       0.0059  ngram   1       0.0121  11:0 15:0
death       2       2       0.0059  ngram   1       0.0121  11:5 23:7
who 2       2       0.0059  ngram   1       0.0121  15:1 21:5
.   4       4       0.0118  ngram   1       0.0242  5:6 9:4 10:6 13:4
be  4       4       0.0118  ngram   1       0.0242  1:1 1:5 9:2 35:3
flee        2       2       0.0059  ngram   1       0.0121  36:1 36:5
not to      4       8       0.0235  ngram   2       0.1538  1:3 36:3 37:3 38:3
</pre></div>
</div>
<p>The various columns are:</p>
<ul class="simple">
<li><strong>Pattern</strong> - The actual pattern. Gaps in skipgrams are represented as <tt class="docutils literal"><span class="pre">{*x*}</span></tt> where x is a number representing the size of the skip. Variable-width skipgrams are just <tt class="docutils literal"><span class="pre">{*}</span></tt>.</li>
<li><strong>Occurrence count</strong> - The absolute number of times this pattern occurs</li>
<li><strong>Tokens</strong> - The absolute number of tokens in the corpus that this pattern covers. Longer patterns by definition cover more tokens. This value&#8217;s maximum is <tt class="docutils literal"><span class="pre">occurrencecount</span> <span class="pre">*</span> <span class="pre">n</span></tt>, the value will be smaller if a pattern overlaps itself.</li>
<li><strong>Coverage</strong> - The number of covered tokens, as a fraction of the total number of tokens.</li>
<li><strong>Category</strong> - The type of pattern (ngram, skipgram or flexgram).</li>
<li><strong>Size</strong> - The length of the n-gram or skipgram in words/tokens.</li>
<li><strong>Frequency</strong> - The frequency of the pattern <em>within its category and
size class</em>, so for an ngram of size two, the frequency indicates the
frequency amongst all bigrams.</li>
<li><strong>References</strong> - A space-delimited list of indices in the corpus that correspond to a occurrence of this pattern. Indices are in the form <tt class="docutils literal"><span class="pre">sentence:token</span></tt> where sentence starts at one and token starts at zero. This column is only available for indexed models.</li>
</ul>
</div>
<div class="section" id="creating-a-pattern-model-with-skipgrams-and-or-flexgrams">
<h3>Creating a pattern model with skipgrams and/or flexgrams<a class="headerlink" href="#creating-a-pattern-model-with-skipgrams-and-or-flexgrams" title="Permalink to this headline">¶</a></h3>
<p>The pattern model created in the previous example did not yet include skip-grams, these have to be explicitly enabled with the <tt class="docutils literal"><span class="pre">-s</span></tt> flag. When this is used, another options becomes available for consideration:</p>
<ul class="simple">
<li><tt class="docutils literal"><span class="pre">-T</span> <span class="pre">[value]</span></tt> - Only skipgrams that have at least this many different types
as skip content, i.e. possible options filling the gaps, will be considered.
The default is set to two.</li>
</ul>
<p>Here is an example of generating an indexed pattern model including skipgrams:</p>
<div class="highlight-python"><div class="highlight"><pre>$ colibri-patternmodeller -f yourcorpus.colibri.dat -t 10 -s -T 3 -o yourcorpus.colibri.indexedpatternmodel
</pre></div>
</div>
<p>If you want to generate unindexed models, simply add the flag <tt class="docutils literal"><span class="pre">-u</span></tt>. Do note
that for unindexed models the parameter <tt class="docutils literal"><span class="pre">-T</span></tt> has no effect, it will extract
all skipgrams it can find as if <tt class="docutils literal"><span class="pre">-T</span></tt> were set to one! If you want decent
skpigrams, you&#8217;re best off with an indexed model. Note that indexed models can
always be read and printed in an unindexed way (with the <tt class="docutils literal"><span class="pre">-u</span></tt> flag); but
unindexed models can not be read in an indexed way, as they simply lack
indices:</p>
<div class="highlight-python"><div class="highlight"><pre>$ colibri-patternmodeller -i yourcorpus.colibri.indexedpatternmodel -c yourcorpus.colibri.cls -u -P
$ colibri-patternmodeller -i yourcorpus.colibri.unindexedpatternmodel -c yourcorpus.colibri.cls -u -P
</pre></div>
</div>
<p>Flexgrams, non-consecutive patterns in which the gap (only one in the current implementation) is of dynamic width, can be generated in one of two ways:</p>
<blockquote>
<div><ul class="simple">
<li>Extract flexgrams by abstracting from skipgrams: use the <tt class="docutils literal"><span class="pre">-S</span> <span class="pre">S</span></tt> flag.</li>
<li>Extract flexgrams directly from n-gram co-occurence: use the <tt class="docutils literal"><span class="pre">-S</span>
<span class="pre">[threshold]</span></tt> flag, where the threshold is expressed as normalised pointwise mutual
information [-1,1].</li>
</ul>
</div></blockquote>
<p>The skipgram approach has the advantage of allowing you to rely on the <tt class="docutils literal"><span class="pre">-T</span></tt>
threshold, but comes with the disadvantage of having a maximum span. The
co-occurrence approach allows for flexgrams over larger distances. Both methods
come at the cost of more memory, especially the former method.</p>
<p>Neither skipgrams nor flexgrams will cross the line boundary of the original
corpus data, so ensure your data is segmented into lines suitable for your
purposes in the encoding stage.</p>
</div>
<div class="section" id="two-stage-building">
<h3>Two-stage building<a class="headerlink" href="#two-stage-building" title="Permalink to this headline">¶</a></h3>
<p>Generating an indexed pattern model takes considerably more memory than an
unindexed model, as instead of mere counts, all indices have to be retained.</p>
<p>The creation of a pattern models progresses through stages of counting and
pruning. When construction of an indexed model with an occurrence threshold of
2 or higher reaches the limits of your system&#8217;s memory capacity, then two-stage building
<em>may</em> offer a solution.</p>
<p>Two-stage building first constructs an unindexed model (demanding less memory),
and subsequently loads this model and searches the corpus for indices for all
patterns in the model. Whilst this method is more time-consuming, it prevents
the memory bump (after counting, prior to pruning) that normal one-stage
building of indexed models have. Two-stage building is enabled using the <tt class="docutils literal"><span class="pre">-2</span></tt> flag:</p>
<div class="highlight-python"><div class="highlight"><pre>$ colibri-patternmodeller -2 -f yourcorpus.colibri.dat -t 10 -s -T 3 -o yourcorpus.colibri.indexedpatternmodel
</pre></div>
</div>
</div>
<div class="section" id="statistical-reports-and-histograms">
<h3>Statistical reports and histograms<a class="headerlink" href="#statistical-reports-and-histograms" title="Permalink to this headline">¶</a></h3>
<p>If you have a pattern model, you can generate a statistical report which includes information on the number of occurrences and number of types for patterns, grouped for n-grams or skipgrams for a specific value of <em>n</em>. A report is generated using the <tt class="docutils literal"><span class="pre">-R</span></tt> flag, the input model is specified using <tt class="docutils literal"><span class="pre">-i</span></tt>:</p>
<div class="highlight-python"><div class="highlight"><pre>$ colibri-patternmodeller -i yourcorpus.colibri.indexedpatternmodel -R
</pre></div>
</div>
<p>Example output:</p>
<div class="highlight-python"><div class="highlight"><pre>REPORT
----------------------------------
                            PATTERNS    TOKENS  COVERAGE     TYPES
Total:                             -       340         -       177
Uncovered:                         -       175    0.5147       136
Covered:                          69       165    0.4853        41

  CATEGORY N (SIZE)   PATTERNS    TOKENS  COVERAGE     TYPES OCCURRENCES
       all       all        69       165    0.4853        41         243
       all         1        40       165    0.4853        40         165
       all         2        11        26    0.0765        13          26
       all         3         7        17    0.0500         9          19
       all         4         5        10    0.0294         9          14
       all         5         5         9    0.0265         9          17
       all         6         1         2    0.0059         6           2
    n-gram       all        62       165    0.4853        40         215
    n-gram         1        40       165    0.4853        40         165
    n-gram         2        11        26    0.0765        13          26
    n-gram         3         5        12    0.0353         8          12
    n-gram         4         3         6    0.0176         6           6
    n-gram         5         2         4    0.0118         6           4
    n-gram         6         1         2    0.0059         6           2
  skipgram       all         7         7    0.0206         6          28
  skipgram         3         2         7    0.0206         4           7
  skipgram         4         2         4    0.0118         4           8
  skipgram         5         3         5    0.0147         5          13
</pre></div>
</div>
<p>Some explanation is in order to correctly interpret this data.  First of all
patterns are grouped by category (ngram,skipgram, flexgram) and size. There are various metrics:</p>
<blockquote>
<div><ul class="simple">
<li><strong>Pattern</strong> - The number of distinct patterns in this group, so for
category n-gram of 2, this reflects the number of distinct bigrams.</li>
<li><strong>Tokens</strong> - The number of tokens that is covered by the patterns in the
group. Longer patterns by definition cover more tokens.
. This is only available for indexed models, for unindexed models it is either omitted or the
number shown is maximum projection <tt class="docutils literal"><span class="pre">occurrencecount</span> <span class="pre">*</span> <span class="pre">size</span></tt> .</li>
<li><strong>Coverage</strong> - The number of tokens covered as a fraction of the total number of tokens. Only for indexed models.</li>
<li><strong>Types</strong> - The number of unique <strong>word</strong> types covered, i.e the number
of distinct unigrams.</li>
<li><strong>Occurrences</strong> - Cumulative occurrence count of all the patterns in
the group. Used as a basis for computing frequency. Occurrence count
differs from <strong>tokens</strong>, the former expresses the number of times a
pattern occurs in the corpus, the latter expresses how many tokens are
part of the pattern</li>
</ul>
</div></blockquote>
<p>To better understand these metrics, let&#8217;s perceive them in the following test
sentence:</p>
<div class="highlight-python"><div class="highlight"><pre>to be or not to be , that is the question
</pre></div>
</div>
<p>If we generate an indexed pattern model purely on this sentence, <strong>with threshold two</strong>. We find the following three patterns:</p>
<div class="highlight-python"><div class="highlight"><pre>PATTERN COUNT   TOKENS  COVERAGE        CATEGORY        SIZE    FREQUENCY REFERENCES
to      2       2       0.181818        ngram            1       0.5     1:0 1:4
be      2       2       0.181818        ngram            1       0.5     1:1 1:5
to be   2       4       0.363636        ngram            2       1       1:0 1:4
</pre></div>
</div>
<p>The report then looks as follows:</p>
<div class="highlight-python"><div class="highlight"><pre>REPORT
----------------------------------
                            PATTERNS    TOKENS  COVERAGE     TYPES
Total:                             -        11         -         9
Uncovered:                         -         7    0.6364         7
Covered:                           3         4    0.3636         2

CATEGORY N (SIZE)   PATTERNS    TOKENS  COVERAGE     TYPES OCCURRENCES
    all       all         3         4    0.3636         2           6
    all         1         2         4    0.3636         2           4
    all         2         1         4    0.3636         2           2
 n-gram       all         3         4    0.3636         2           6
 n-gram         1         2         4    0.3636         2           4
 n-gram         2         1         4    0.3636         2           2
</pre></div>
</div>
<p>Our sentence has 11 tokens, 7 of which are not covered by the patterns found, 4
of which are.  Since we have only n-grams and no skipgrams or flexgrams in this
simple example, the data for <em>all</em> and <em>n-gram</em> is the same. The <strong>coverage</strong>
metric expresses this in a normalised fashion.</p>
<p>In our data we have two unigrams <em>(to, be)</em> and one bigram <em>(to be)</em>, this is
expressed by the <strong>patterns</strong> metric. Both the unigrams and the bigrams cover
the exact same four tokens in our sentence, i.e  0, 1, 4, and 5, so the TOKENS
column reports four for all. If we look at the <strong>types</strong> column, we notice we
only have two word types: <em>to</em> and <em>be</em>. The unigrams occur in four different
instances and the bigrams occur in two different instances. This is expressed
in the <strong>occurrences</strong> column. Combined that makes six occurrences.</p>
<p>We have 9 types in total, of which only 2 (to, be) are covered, the remaining 7
<em>(or not , that is the question)</em> remain uncovered as we set our occurrence threshold for
this model to two.</p>
<p>Pattern models store how many of the tokens and types in the original corpus
were covered. Tokens and types not covered did not make the set thresholds.
Make sure to use indexed models if you want accurate coverage data.</p>
<p>A histogram can also be generated, using the <tt class="docutils literal"><span class="pre">-H</span></tt> flag:</p>
<div class="highlight-python"><div class="highlight"><pre>$ colibri-patternmodeller -i yourcorpus.colibri.indexedpatternmodel -H
</pre></div>
</div>
<p>Example output:</p>
<div class="highlight-python"><div class="highlight"><pre>OCCURRENCES   PATTERNS
2   39
3   5
4   13
5   5
6   1
7   1
8   1
10  1
13  1
14  1
15  1
</pre></div>
</div>
</div>
<div class="section" id="filtering-models">
<h3>Filtering models<a class="headerlink" href="#filtering-models" title="Permalink to this headline">¶</a></h3>
<p>Patterns models can be read with <tt class="docutils literal"><span class="pre">-i</span></tt> and filtered by setting stricter thresholds prior to printing, reporting or outputting to file. An example:</p>
<div class="highlight-python"><div class="highlight"><pre>$ colibri-patternmodeller -i yourcorpus.colibri.indexedpatternmodel -t 20 -T 10 -o yourcorpus_filtered.colibri.indexedpatternmodel -P
</pre></div>
</div>
<p>You can also filter pattern models by intersecting with another pattern model
using the <tt class="docutils literal"><span class="pre">-j</span></tt> option. This only works when both are built on
the same class file:</p>
<div class="highlight-python"><div class="highlight"><pre>$ colibri-patternmodeller -i yourcorpus.colibri.indexedpatternmodel -j yourcorpus2.colibri.indexedpatternmodel -o yourcorpus_filtered.colibri.indexedpatternmodel
</pre></div>
</div>
<p>The output pattern model will contain only those patterns that were present in
both the input model (<tt class="docutils literal"><span class="pre">-i</span></tt>) as well as the constraining model (<tt class="docutils literal"><span class="pre">-j</span></tt>), which may be
either indexed or unindexed regardless of the input model; it will
always contain the counts/indices from the input model.</p>
</div>
<div class="section" id="training-and-testing-coverage">
<h3>Training and testing coverage<a class="headerlink" href="#training-and-testing-coverage" title="Permalink to this headline">¶</a></h3>
<p>An important quality of pattern models lies in the fact that pattern models can
be compared, provided they use comparable vocabulary, i.e. are based on the same
class file.  More specifically, you can train a pattern model on a corpus and
test it on another corpus, which yields another pattern model containing only
those patterns that occur in both training and test data. The difference in
count, frequency and coverage can then be easily be compared. You build such a
model by taking the intersection with a training model using the <tt class="docutils literal"><span class="pre">-j</span></tt> flag.
Make sure to always use the same class file for all datasets you are comparing.
Instructions for this were given in <em class="xref std std-ref">classencodetraintest</em>.</p>
<dl class="docutils">
<dt>Training::</dt>
<dd>$ colibri-patternmodeller -f trainset.colibri.dat -o trainset.colibri.indexedpatternmodel</dd>
</dl>
<p>This results in a model <tt class="docutils literal"><span class="pre">trainset.colibri.indexedpatternmodel</span></tt>. Now proceed
with testing on another corpus:</p>
<dl class="docutils">
<dt>Testing::</dt>
<dd>$ colibri-patternmodeller -f testset.colibri.dat -j trainset.indexedpatternmodel.colibri -o testset.colibri.indexedpatternmodel</dd>
</dl>
<p>or (more memory efficient):</p>
<div class="highlight-python"><div class="highlight"><pre>$ colibri-patternmodeller -f testset.colibri.dat -I trainset.indexedpatternmodel.colibri -o testset.colibri.indexedpatternmodel
</pre></div>
</div>
<p>This results in a model <tt class="docutils literal"><span class="pre">testset.colibri.indexedpatternmodel</span></tt> that only contains patterns that also occur in the specified training model.</p>
<p>Such an intersection of models can also be created at any later stage using
<tt class="docutils literal"><span class="pre">-i</span></tt> and <tt class="docutils literal"><span class="pre">-j</span></tt>, as shown in the previous section.</p>
</div>
<div class="section" id="reverse-index">
<h3>Reverse index<a class="headerlink" href="#reverse-index" title="Permalink to this headline">¶</a></h3>
<p>Indexed pattern models have a what is called a <em>forward index</em>. For each
pattern, all of the positions, <tt class="docutils literal"><span class="pre">(sentence,token)</span></tt>, at which a token of the
pattern can be found, is held. In Colibri-core, sentences always start at 1, whereas
tokens start at 0.</p>
<p>A reverse index is a mapping of references of the type <tt class="docutils literal"><span class="pre">(sentence,token)</span></tt> to a set of all the
patterns that <em>begin</em> at that location. Such a reverse index can be constructed
from the forward index of an indexed pattern model, or it can be explicitly
given by simply passing the original corpus data to the model, which makes
reverse indices available even for unindexed models. Explicitly providing a
reverse index makes loading a model faster (especially on larger models), but at
the cost of higher memory usage, especially in case of sparse models.</p>
<p>Passing corpus data for the reverse index to colibri-patternmodeller is done
using the <tt class="docutils literal"><span class="pre">-r</span></tt> flag, and the full reverse index can be displayed using the <tt class="docutils literal"><span class="pre">-Z</span></tt> flag:</p>
<div class="highlight-python"><div class="highlight"><pre>$ colibri-patternmodeller  -i yourcorpus.indexedpatternmodel.colibri -r yourcorpus.colibri.dat -c yourcorpus.colibri.cls -Z
</pre></div>
</div>
<p>Indexes and/or reverse indexes are required for various purposes, one of which
is the extraction of relations and co-occurrence information.</p>
</div>
<div class="section" id="query-mode">
<h3>Query mode<a class="headerlink" href="#query-mode" title="Permalink to this headline">¶</a></h3>
<p>The pattern modeller has query mode which allows you to quickly extract patterns from test sentences or fragments thereof. The query mode is invoked by loading a pattern model (<tt class="docutils literal"><span class="pre">-i</span></tt>), a class file (<tt class="docutils literal"><span class="pre">-c</span></tt>) and the <tt class="docutils literal"><span class="pre">-Q</span></tt> flag. The query mode can be run interactively as it takes input from <tt class="docutils literal"><span class="pre">stdin</span></tt>, one <em>tokenised</em> sentence per line. The following example illustrates this, the sentence <em>&#8220;To be or not to be&#8221;</em> was typed as input:</p>
<div class="highlight-python"><div class="highlight"><pre>$ colibri-patternmodeller -i /tmp/data.colibri.patternmodel -c /tmp/hamlet.colibri.cls -Q
Loading class decoder from file /tmp/hamlet.colibri.cls
Loading class encoder from file /tmp/hamlet.colibri.cls
Loading indexed pattern model /tmp/data.colibri.patternmodel as input model...
Colibri Patternmodeller -- Interactive query mode.
Type ctrl-D to quit, type X to switch between exact mode and extensive mode (default: extensive mode).
1&gt;&gt; To be or not to be
1:0 To      8               8       0.0235294       ngram   1       0.0484848   1:0 5:7 9:5 10:0 22:0 36:0 37:0 38:0
1:1 be      4               4       0.0117647       ngram   1       0.0242424   1:1 1:5 9:2 35:3
1:2 or      4               4       0.0117647       ngram   1       0.0242424   1:2 36:2 37:2 38:2
1:3 not     5               5       0.0147059       ngram   1       0.030303    1:3 27:7 36:3 37:3 38:3
1:4 to      13              13      0.0382353       ngram   1       0.0787879   1:4 2:6 4:1 5:10 6:7 8:4 9:1 9:8 10:4 27:2 36:4 37:4 38:4
1:2 or not  4               8       0.0235294       ngram   2       0.153846    1:2 36:2 37:2 38:2
1:3 not to  4               8       0.0235294       ngram   2       0.153846    1:3 36:3 37:3 38:3
1:2 or not to       4               12      0.0352941       ngram   3       0.333333    1:2 36:2 37:2 38:2
</pre></div>
</div>
<p>The output starts with an index in the format <tt class="docutils literal"><span class="pre">sentence:token</span></tt>, specifying
where the pattern found was found in your input. The next columns are the same
as the print output.The interactive query mode distinguishes two modes,
extensive mode and exact mode. In extensive mode, your input string will be
scanned for all patterns occurring in it. In exact mode, the input you
specified needs to match exactly and as a whole. Type <tt class="docutils literal"><span class="pre">X</span></tt> to switch between
the modes.</p>
<p>In addition to interactive query mode, there is also a command line query mode
<tt class="docutils literal"><span class="pre">-q</span></tt> in which you specify the pattern you want to query as argument on the command
line. Multiple patterns can be specified by repeating the <tt class="docutils literal"><span class="pre">-q</span></tt> flag. This
mode always behaves according to exact mode:</p>
<div class="highlight-python"><div class="highlight"><pre>$ colibri-patternmodeller -i /tmp/data.colibri.patternmodel -c /tmp/hamlet.colibri.cls -q &quot;to be&quot;
Loading class decoder from file /tmp/hamlet.colibri.cls
Loading class encoder from file /tmp/hamlet.colibri.cls
Loading indexed pattern model /tmp/data.colibri.patternmodel as input model...
to be       2               4       0.0117647       ngram   2       0.0769231   1:4 9:1
</pre></div>
</div>
</div>
<div class="section" id="pattern-relations">
<h3>Pattern Relations<a class="headerlink" href="#pattern-relations" title="Permalink to this headline">¶</a></h3>
<p>A pattern model contains a wide variety of patterns; the relationships between those can be made explicit. These relationships can be imagined as a directed graph, in which the nodes represent the various patterns (n-grams and skipgrams), and the edges represent the relations. The following relations are distinguished; note that as the graph is directed relations often come in pairs; one relationship for each direction:</p>
<ul class="simple">
<li><strong>Subsumption relations</strong> - Patterns that are subsumed by larger patterns are called <em>subsumption children</em>, the larger patterns are called <em>subsumption parents</em>. These are the two subsumption relations that can be extracted from an indexed pattern model.</li>
<li><strong>Successor relations</strong>  - Patterns that follow eachother are in a left-of/right-of relation.</li>
<li><strong>Instantiation relations</strong> - There is a relation between skipgrams and
patterns that instantiate them <tt class="docutils literal"><span class="pre">to</span> <span class="pre">be</span> <span class="pre">{*1*}</span> <span class="pre">not</span> <span class="pre">{*1*}</span> <span class="pre">be</span></tt> is instantiated
by <tt class="docutils literal"><span class="pre">to</span> <span class="pre">{*1*}</span> <span class="pre">or</span></tt>, also referred to as the skip content.</li>
</ul>
<p>You can all of these extract relations using the <tt class="docutils literal"><span class="pre">-g</span></tt> flag, which is to be
used in combination with the query mode <tt class="docutils literal"><span class="pre">-Q</span></tt> or <tt class="docutils literal"><span class="pre">-q</span></tt>. Consider the
following sample:</p>
<div class="highlight-python"><div class="highlight"><pre>$ colibri-patternmodeller -i /tmp/data.colibri.patternmodel -c /tmp/hamlet.colibri.cls -q &quot;to be&quot; -g
Loading class decoder from file /tmp/hamlet.colibri.cls
Loading class encoder from file /tmp/hamlet.colibri.cls
Loading indexed pattern model /tmp/data.colibri.patternmodel as input model...
Post-read processing (indexedmodel)
to be       2               4       0.0117647       ngram   2       0.0769231       1:4 9:1
#   PATTERN1        RELATION        PATTERN2        REL.COUNT       REL.FREQUENCY   COUNT2
    to be   SUBSUMES        to      2       0.5     13
    to be   SUBSUMES        be      2       0.5     4
    to be   RIGHT-NEIGHBOUR-OF      To {*1*} or not 1       0.25    4
    to be   RIGHT-NEIGHBOUR-OF      To {*2*} not    1       0.25    4
    to be   RIGHT-NEIGHBOUR-OF      not     1       0.25    5
    to be   RIGHT-NEIGHBOUR-OF      or not  1       0.25    4
</pre></div>
</div>
<p>The following columns are reported, all are indented with a single tab so
possible parsers can distinguish the numbers for the queried pattern itself from the relationships with other patterns.</p>
<blockquote>
<div><ul class="simple">
<li><strong>Pattern 1</strong> &#8211; The pattern you queried</li>
<li><strong>Relation</strong> &#8211; The nature of the relationship between pattern 1 and pattern 2</li>
<li><strong>Pattern 2</strong> &#8211; The pattern that is related to the queried pattern</li>
<li><strong>Relation Count</strong> &#8211; The number of times pattern 1 and pattern 2 occur in this relation</li>
<li><strong>Relation Frequency</strong> &#8211; The number of times pattern 1 and pattern 2 occur in this relationas a fraction of all relations of this type</li>
<li><strong>Count 2</strong> &#8211; The absolute number of occurrences of pattern 2 in the model</li>
</ul>
</div></blockquote>
</div>
<div class="section" id="co-occurrence">
<h3>Co-occurrence<a class="headerlink" href="#co-occurrence" title="Permalink to this headline">¶</a></h3>
<p>Co-occurrence in Colibri-Core measures which patterns co-occur on the same line
(i.e. usually corresponding to a sentence or whatever structural unit you
decided upon when encoding your corpus). Co-occurrence is another relation in
addition to the ones described in the previous section.</p>
<p>The degree of co-occurrence can be expressed as either an absolute occurrence
number (<tt class="docutils literal"><span class="pre">-C</span></tt>), for a normalised mutual pointwise information (<tt class="docutils literal"><span class="pre">-Y</span></tt>). Both
flags take a threshold, setting the threshold too low, specially for npmi, may
cause very high memory usage. The following syntax would show all patterns that
occur at least five times in the same sentence. Note that the order of the
pattern pairs does not matter; if there are two patterns X and Y then result X
Y or Y X would be the same, yet only one of them is included in the output to
prevent duplicating information:</p>
<div class="highlight-python"><div class="highlight"><pre>$ colibri-patternmodeller -i /tmp/data.colibri.patternmodel -c /tmp/hamlet.colibri.cls -C 5
</pre></div>
</div>
</div>
</div>
<div class="section" id="architecture-overview">
<h2>Architecture Overview<a class="headerlink" href="#architecture-overview" title="Permalink to this headline">¶</a></h2>
<img alt="_images/arch.png" src="_images/arch.png" />
</div>
<div class="section" id="python-tutorial">
<h2>Python Tutorial<a class="headerlink" href="#python-tutorial" title="Permalink to this headline">¶</a></h2>
<p>Colibri Core offers both a C++ API as well as a Python API. It exposes all of
the functionality, and beyond, of the tools outlined above. The Python API
binds with the C++ code, and although it is more limited than the C++ API, it
still offers most higher-level functionality. The Colibri Core binding between C++ and
Python is written in Cython.</p>
<p>A Python tutorial for Colibri Core is available in the form of an IPython
Notebook, meaning that you can interactively run it and play with. You can go
to the static, read-only, version <a class="reference external" href="http://proycon.github.io/colibri-core/doc/colibricore-python-tutorial.html">by clicking here</a></p>
</div>
<div class="section" id="python-api-reference">
<h2>Python API Reference<a class="headerlink" href="#python-api-reference" title="Permalink to this headline">¶</a></h2>
</div>
</div>


          </div>
        </div>
      </div>
      <div class="sphinxsidebar">
        <div class="sphinxsidebarwrapper">
  <h3><a href="#">Table Of Contents</a></h3>
  <ul>
<li><a class="reference internal" href="#">Colibri Documentation</a><ul>
<li><a class="reference internal" href="#introduction">Introduction</a></li>
<li><a class="reference internal" href="#installation">Installation</a><ul>
<li><a class="reference internal" href="#keeping-colibri-up-to-date">Keeping colibri up to date</a></li>
<li><a class="reference internal" href="#general-usage-instructions">General usage instructions</a></li>
</ul>
</li>
<li><a class="reference internal" href="#quick-start-high-level-scripts">Quick start: High-level scripts</a><ul>
<li><a class="reference internal" href="#id1">Introduction</a></li>
<li><a class="reference internal" href="#tokenisation">Tokenisation</a></li>
<li><a class="reference internal" href="#scripts">Scripts</a></li>
</ul>
</li>
<li><a class="reference internal" href="#corpus-class-encoding">Corpus Class Encoding</a><ul>
<li><a class="reference internal" href="#id2">Introduction</a></li>
<li><a class="reference internal" href="#class-encoding-your-corpus">Class-encoding your corpus</a></li>
<li><a class="reference internal" href="#class-decoding-your-corpus">Class-decoding your corpus</a></li>
</ul>
</li>
<li><a class="reference internal" href="#pattern-modeller">Pattern Modeller</a><ul>
<li><a class="reference internal" href="#id4">Introduction</a></li>
<li><a class="reference internal" href="#creating-a-pattern-model">Creating a pattern model</a></li>
<li><a class="reference internal" href="#creating-a-pattern-model-with-skipgrams-and-or-flexgrams">Creating a pattern model with skipgrams and/or flexgrams</a></li>
<li><a class="reference internal" href="#two-stage-building">Two-stage building</a></li>
<li><a class="reference internal" href="#statistical-reports-and-histograms">Statistical reports and histograms</a></li>
<li><a class="reference internal" href="#filtering-models">Filtering models</a></li>
<li><a class="reference internal" href="#training-and-testing-coverage">Training and testing coverage</a></li>
<li><a class="reference internal" href="#reverse-index">Reverse index</a></li>
<li><a class="reference internal" href="#query-mode">Query mode</a></li>
<li><a class="reference internal" href="#pattern-relations">Pattern Relations</a></li>
<li><a class="reference internal" href="#co-occurrence">Co-occurrence</a></li>
</ul>
</li>
<li><a class="reference internal" href="#architecture-overview">Architecture Overview</a></li>
<li><a class="reference internal" href="#python-tutorial">Python Tutorial</a></li>
<li><a class="reference internal" href="#python-api-reference">Python API Reference</a></li>
</ul>
</li>
</ul>

  <h3>This Page</h3>
  <ul class="this-page-menu">
    <li><a href="_sources/index.txt"
           rel="nofollow">Show Source</a></li>
  </ul>
<div id="searchbox" style="display: none">
  <h3>Quick search</h3>
    <form class="search" action="search.html" method="get">
      <input type="text" name="q" />
      <input type="submit" value="Go" />
      <input type="hidden" name="check_keywords" value="yes" />
      <input type="hidden" name="area" value="default" />
    </form>
    <p class="searchtip" style="font-size: 90%">
    Enter search terms or a module, class or function name.
    </p>
</div>
<script type="text/javascript">$('#searchbox').show(0);</script>
        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="related">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="genindex.html" title="General Index"
             >index</a></li>
        <li><a href="#">Colibri Core 0.5.3 documentation</a> &raquo;</li> 
      </ul>
    </div>
    <div class="footer">
        &copy; Copyright 2013, Maarten van Gompel.
      Created using <a href="http://sphinx-doc.org/">Sphinx</a> 1.2.2.
    </div>
  </body>
</html>