{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Colibri Core Python Tutorial"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "*by Maarten van Gompel, Radboud University Nijmegen*"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This tutorial will show you how to work with Colibri Core's Python API. It is assumed that you have already read the Colibri Core documentation, followed the installation instructions, and are familiar its purpose and concepts. The documentation also provides an API reference for all the Python classes and method. This tutorial is in the form of a Python Notebook, allowing you to interactively participate. Press ``shift+enter`` in code field to evaluate it.\n",
      "\n",
      "Colibri Core is written in C++ and the Python binding is writting in Cython. This offers the advantage of native-speed and memory efficiency, combined with the ease of a high-level pythonic interface. We will be using Python 3."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We obviously start our adventure with an import of colibricore, so make sure you installed it properly:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import colibricore\n",
      "\n",
      "TMPDIR = \"/tmp/\" #this is where we'll store intermediate files\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Class encoding/decoding"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "To give us something to work with, we will take an excerpt of Shakespeare's Hamlet as our corpus text:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "corpustext = \"\"\"To be, or not to be, that is the question\n",
      "Whether 'tis Nobler in the mind to suffer\n",
      "The Slings and Arrows of outrageous Fortune,\n",
      "Or to take Arms against a Sea of troubles,\n",
      "And by opposing end them? To die, to sleep\n",
      "No more; and by a sleep, to say we end\n",
      "The Heart-ache, and the thousand Natural shocks\n",
      "That Flesh is heir to? 'Tis a consummation\n",
      "Devoutly to be wished. To die, to sleep,\n",
      "To sleep, perchance to Dream; Aye, there's the rub,\n",
      "For in that sleep of death, what dreams may come,\n",
      "When we have shuffled off this mortal coil,\n",
      "Must give us pause. There's the respect\n",
      "That makes Calamity of so long life:\n",
      "For who would bear the Whips and Scorns of time,\n",
      "Th' Oppressor's wrong, the proud man's Contumely,\n",
      "The pangs of despised Love, the Law\u2019s delay,\n",
      "The insolence of Office, and the Spurns\n",
      "That patient merit of the unworthy takes,\n",
      "When he himself might his Quietus make\n",
      "With a bare Bodkin? Who would these Fardels bear,\n",
      "To grunt and sweat under a weary life,\n",
      "But that the dread of something after death,\n",
      "The undiscovered Country, from whose bourn\n",
      "No Traveler returns, Puzzles the will,\n",
      "And makes us rather bear those ills we have,\n",
      "Than fly to others that we know not of.\n",
      "Thus Conscience does make Cowards of us all,\n",
      "And thus the Native hue of Resolution\n",
      "Is sicklied o'er, with the pale cast of Thought,\n",
      "And enterprises of great pitch and moment,\n",
      "With this regard their Currents turn awry,\n",
      "And lose the name of Action. Soft you now,\n",
      "The fair Ophelia. Nymph, in all thy Orisons\n",
      "Be all my sins remembered\"\"\"\n",
      "\n",
      "#first we do some very rudimentary tokenisation\n",
      "corpustext = corpustext.replace(',',' ,')\n",
      "corpustext = corpustext.replace('.',' .')\n",
      "corpustext = corpustext.replace(':',' :')\n",
      "\n",
      "\n",
      "corpusfile_plaintext = TMPDIR + \"hamlet.txt\"\n",
      "\n",
      "with open(corpusfile_plaintext,'w',encoding='utf-8') as f:\n",
      "    f.write(corpustext)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "To work with this data with Colibri Core. We need to *class encode* it, assigning integer values to each word type. Using Python, a class encoder is built as follows:\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "classfile = TMPDIR + \"hamlet.colibri.cls\"\n",
      "\n",
      "#Instantiate class encoder\n",
      "classencoder = colibricore.ClassEncoder()\n",
      "\n",
      "#Build classes\n",
      "classencoder.build(corpusfile_plaintext)\n",
      "\n",
      "#Save class file\n",
      "classencoder.save(classfile)\n",
      "\n",
      "print(\"Encoded \", len(classencoder), \" classes, well done!\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Encoded  184  classes, well done!\n"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now we have a class encoder we can encode our corpus:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "corpusfile = TMPDIR + \"hamlet.colibri.dat\" #this will be the encoded corpus file\n",
      "classencoder.encodefile(corpusfile_plaintext, corpusfile)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "To check whether that worked as planned, we will construct a Class Decoder, load our class file, and decode the corpus:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Load class decoder from the classfile we just made\n",
      "classdecoder = colibricore.ClassDecoder(classfile)\n",
      "\n",
      "#Decode corpus data\n",
      "decoded = classdecoder.decodefile(corpusfile)\n",
      "\n",
      "#Show\n",
      "print(decoded)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "To be , or not to be , that is the question\n",
        "Whether 'tis Nobler in the mind to suffer\n",
        "The Slings and Arrows of outrageous Fortune ,\n",
        "Or to take Arms against a Sea of troubles ,\n",
        "And by opposing end them? To die , to sleep\n",
        "No more; and by a sleep , to say we end\n",
        "The Heart-ache , and the thousand Natural shocks\n",
        "That Flesh is heir to? 'Tis a consummation\n",
        "Devoutly to be wished . To die , to sleep ,\n",
        "To sleep , perchance to Dream; Aye , there's the rub ,\n",
        "For in that sleep of death , what dreams may come ,\n",
        "When we have shuffled off this mortal coil ,\n",
        "Must give us pause . There's the respect\n",
        "That makes Calamity of so long life :\n",
        "For who would bear the Whips and Scorns of time ,\n",
        "Th' Oppressor's wrong , the proud man's Contumely ,\n",
        "The pangs of despised Love , the Law\u2019s delay ,\n",
        "The insolence of Office , and the Spurns\n",
        "That patient merit of the unworthy takes ,\n",
        "When he himself might his Quietus make\n",
        "With a bare Bodkin? Who would these Fardels bear ,\n",
        "To grunt and sweat under a weary life ,\n",
        "But that the dread of something after death ,\n",
        "The undiscovered Country , from whose bourn\n",
        "No Traveler returns , Puzzles the will ,\n",
        "And makes us rather bear those ills we have ,\n",
        "Than fly to others that we know not of .\n",
        "Thus Conscience does make Cowards of us all ,\n",
        "And thus the Native hue of Resolution\n",
        "Is sicklied o'er , with the pale cast of Thought ,\n",
        "And enterprises of great pitch and moment ,\n",
        "With this regard their Currents turn awry ,\n",
        "And lose the name of Action . Soft you now ,\n",
        "The fair Ophelia . Nymph , in all thy Orisons\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Playing with patterns"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now we have a class encoder and decoder, we call toy around with the most basic units in Colibri Core: patterns. You would use an instance of ``Pattern`` where you'd normally use a string, as Patterns are much smaller in memory. Let's build a pattern from a string using the classencoder, note that we will only be able to use words that are known by the class encoder:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Build a pattern from a string, using the class encoder\n",
      "p = classencoder.buildpattern(\"To be or not to be\")\n",
      "\n",
      "#To print it we need the decoder\n",
      "print(p.tostring(classdecoder))\n",
      "print(len(p))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "To be or not to be\n",
        "6\n"
       ]
      }
     ],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Iterate over the token in a pattern, each token will be a Pattern instance\n",
      "\n",
      "for token in p:\n",
      "    print(token.tostring(classdecoder))\n",
      "    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "To\n",
        "be\n",
        "or\n",
        "not\n",
        "to\n",
        "be\n"
       ]
      }
     ],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Extracting subpatterns by offset\n",
      "\n",
      "#Get first token\n",
      "print(p[0].tostring(classdecoder))\n",
      "\n",
      "#Get last token\n",
      "print(p[-1].tostring(classdecoder))\n",
      "\n",
      "#Get slice\n",
      "print(p[2:4].tostring(classdecoder))\n",
      "    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "To\n",
        "be\n",
        "or not\n"
       ]
      }
     ],
     "prompt_number": 9
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Given a pattern, we can easily extract all n-grams in it:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#let's get all bigrams\n",
      "for ngram in p.ngrams(2):\n",
      "    print(ngram.tostring(classdecoder))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "To be\n",
        "be or\n",
        "or not\n",
        "not to\n",
        "to be\n"
       ]
      }
     ],
     "prompt_number": 10
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#or all n-grams:\n",
      "for ngram in p.ngrams():\n",
      "    print(ngram.tostring(classdecoder))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "To\n",
        "be\n",
        "or\n",
        "not\n",
        "to\n",
        "be\n",
        "To be\n",
        "be or\n",
        "or not\n",
        "not to\n",
        "to be\n",
        "To be or\n",
        "be or not\n",
        "or not to\n",
        "not to be\n",
        "To be or not\n",
        "be or not to\n",
        "or not to be\n",
        "To be or not to\n",
        "be or not to be\n"
       ]
      }
     ],
     "prompt_number": 11
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#or particular ngrams, such as unigrams up to trigrams:\n",
      "for ngram in p.ngrams(1,3):\n",
      "    print(ngram.tostring(classdecoder))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "To\n",
        "be\n",
        "or\n",
        "not\n",
        "to\n",
        "be\n",
        "To be\n",
        "be or\n",
        "or not\n",
        "not to\n",
        "to be\n",
        "To be or\n",
        "be or not\n",
        "or not to\n",
        "not to be\n"
       ]
      }
     ],
     "prompt_number": 12
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The in operator can be used to check if a token **OR** ngram is part of a pattern"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#token\n",
      "p2 = classencoder.buildpattern(\"be\")\n",
      "print(p2 in p)\n",
      "\n",
      "#ngram\n",
      "p3 = classencoder.buildpattern(\"or not\")\n",
      "print(p3 in p)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "True\n",
        "True\n"
       ]
      }
     ],
     "prompt_number": 13
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Just to prove that our Pattern representation is usually smaller than a string representation, and offer a sneak peek under the hood:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print(bytes(p), len(bytes(p)))\n",
      "print(b\"To be or not to be\", len(b\"To be or not to be\"))\n",
      "len(bytes(p)) < len(b\"To be or not to be\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "b'\\x01\\x0e\\x01\\x16\\x01\\x83\\x01\\x1e\\x01\\t\\x01\\x16' 12\n",
        "b'To be or not to be' 18\n"
       ]
      },
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 14,
       "text": [
        "True"
       ]
      }
     ],
     "prompt_number": 14
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "If we are merely interested in extracting ngrams by moving a sliding window over all lines in our text. Then we can read the entire corpus as a single pattern, this will however ignore any newlines as a Pattern can not span multiple lines by definition.\n",
      "    "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#TODO"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 15
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Pattern Models"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now it's time to build our first pattern model on the Hamlet excerpt. We will extract all patterns occuring at least twice and with maximum length 8."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Set the options\n",
      "options = colibricore.PatternModelOptions(mintokens=2,maxlength=8)\n",
      "\n",
      "#Instantiate an empty unindexed model \n",
      "model = colibricore.UnindexedPatternModel()\n",
      "\n",
      "#Train it on our corpus file (class-encoded data, not plain text)\n",
      "model.train(corpusfile, options)\n",
      "\n",
      "print(\"Found \" , len(model), \" patterns:\")\n",
      "\n",
      "#Let's see what patterns are in our model (the order will be 'random')\n",
      "for pattern in model:\n",
      "    print(pattern.tostring(classdecoder))\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Found  54  patterns:\n",
        "To die , to sleep\n",
        "To die , to\n",
        ", and the\n",
        ", to sleep\n",
        "die , to\n",
        "To die ,\n",
        ", the\n",
        "and the\n",
        ", and\n",
        "sleep ,\n",
        "to sleep\n",
        "die , to sleep\n",
        ", to\n",
        "die ,\n",
        "we have\n",
        "to be\n",
        "To die\n",
        "be ,\n",
        "all\n",
        "With\n",
        "we\n",
        "sleep\n",
        "That\n",
        "by\n",
        "a\n",
        "make\n",
        "die\n",
        "end\n",
        "to\n",
        "And\n",
        ",\n",
        "be\n",
        "To\n",
        "would\n",
        "this\n",
        "death ,\n",
        "No\n",
        "not\n",
        ".\n",
        "us\n",
        "The\n",
        "For\n",
        "of\n",
        "death\n",
        "bear\n",
        "When\n",
        "and\n",
        "have\n",
        "is\n",
        "in\n",
        "the\n",
        "makes\n",
        "that\n",
        "life\n"
       ]
      }
     ],
     "prompt_number": 16
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Let's see the counts as well, models behave much alike to dictionaries:\n",
      "for pattern, count in model.items():\n",
      "    print(pattern.tostring(classdecoder), count)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "To die , to sleep 2\n",
        "To die , to 2\n",
        ", and the 2\n",
        ", to sleep 2\n",
        "die , to 2\n",
        "To die , 2\n",
        ", the 2\n",
        "and the 2\n",
        ", and 2\n",
        "sleep , 3\n",
        "to sleep 2\n",
        "die , to sleep 2\n",
        ", to 3\n",
        "die , 2\n",
        "we have 2\n",
        "to be 2\n",
        "To die 2\n",
        "be , 2\n",
        "all 2\n",
        "With 2\n",
        "we 4\n",
        "sleep 5\n",
        "That 3\n",
        "by 2\n",
        "a 5\n",
        "make 2\n",
        "die 2\n",
        "end 2\n",
        "to 9\n",
        "And 5\n",
        ", 36\n",
        "be 3\n",
        "To 5\n",
        "would 2\n",
        "this 2\n",
        "death , 2\n",
        "No 2\n",
        "not 2\n",
        ". 5\n",
        "us 3\n",
        "The 6\n",
        "For 2\n",
        "of 15\n",
        "death 2\n",
        "bear 3\n",
        "When 2\n",
        "and 7\n",
        "have 2\n",
        "is 2\n",
        "in 3\n",
        "the 15\n",
        "makes 2\n",
        "that 4\n",
        "life 2\n"
       ]
      }
     ],
     "prompt_number": 17
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We can also query specific patterns:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\n",
      "querypattern = classencoder.buildpattern(\"sleep\")\n",
      "\n",
      "print(\"How much sleep?\")\n",
      "print(model[querypattern])\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "How much sleep?\n",
        "5\n"
       ]
      }
     ],
     "prompt_number": 18
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Like dictionaries, unknown patterns will trigger a KeyError\n",
      "querypattern = classencoder.buildpattern(\"insolence\")\n",
      "\n",
      "print(\"How much insolence?\")\n",
      "try:\n",
      "    print(model[querypattern])\n",
      "except KeyError:\n",
      "    print(\"Nope, KeyError, no such pattern in model..\")\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "How much insolence?\n",
        "Nope, KeyError, no such pattern in model..\n"
       ]
      }
     ],
     "prompt_number": 19
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We can check whether a pattern is in a model in the usual pythonic fashion:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "if querypattern in model:\n",
      "    print(\"No insolence in model!\")\n",
      "else:\n",
      "    print(\"Insolence in model!\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Insolence in model!\n"
       ]
      }
     ],
     "prompt_number": 20
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Rather than the absolute counts, we can get the frequency of a pattern within its type and class. For example the frequency of a bigram amongst all bigrams:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "querypattern = classencoder.buildpattern(\"and the\")\n",
      "\n",
      "print(model.frequency(querypattern))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.07692307692307693\n"
       ]
      }
     ],
     "prompt_number": 21
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Once we have a model, we can save it to file, to reload later, loading is much faster than training:\n",
      "    "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "patternmodelfile = TMPDIR + \"hamlet.colibri.patternmodel\"\n",
      "\n",
      "model.write(patternmodelfile)\n",
      "\n",
      "#and reload just to show we can:\n",
      "model = colibricore.UnindexedPatternModel(patternmodelfile, options)\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 22
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Unindexed models are much smaller in memory than indexed models, but their functionality is also limited. Let's take a look at indexed models:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Set the options\n",
      "options = colibricore.PatternModelOptions(mintokens=2,maxlength=8,doreverseindex=True)\n",
      "\n",
      "#Instantiate an empty unindexed model \n",
      "model = colibricore.IndexedPatternModel()\n",
      "\n",
      "#Train it on our corpus file (class-encoded data, not plain text)\n",
      "model.train(corpusfile, options)\n",
      "\n",
      "print(\"Found \" , len(model), \" patterns:\")\n",
      "\n",
      "#Let's see what patterns are in our model (the order will be 'random')\n",
      "for pattern, indices in model.items():\n",
      "    print(pattern.tostring(classdecoder),end=\" \")\n",
      "    for index in indices:\n",
      "        print(index,end=\" \") #(sentence,token) tuple, sentences start with 1, tokens with 0\n",
      "    print()\n",
      "        "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Found  54  patterns:\n",
        "To die , to sleep (5, 5) (9, 5) \n",
        "To die , to (5, 5) (9, 5) \n",
        ", and the (7, 2) (18, 4) \n",
        ", to sleep (5, 7) (9, 7) \n",
        "die , to (5, 6) (9, 6) \n",
        "To die , (5, 5) (9, 5) \n",
        ", the (16, 3) (17, 5) \n",
        "and the (7, 3) (18, 5) \n",
        ", and (7, 2) (18, 4) \n",
        "sleep , (6, 5) (9, 9) (10, 1) \n",
        "to sleep (5, 8) (9, 8) \n",
        "die , to sleep (5, 6) (9, 6) \n",
        ", to (5, 7) (6, 6) (9, 7) \n",
        "die , (5, 6) (9, 6) \n",
        "we have (12, 1) (26, 7) \n",
        "to be (1, 5) (9, 1) \n",
        "To die (5, 5) (9, 5) \n",
        "be , (1, 1) (1, 6) \n",
        "all (28, 7) (34, 7) \n",
        "With (21, 0) (32, 0) \n",
        "we (6, 9) (12, 1) (26, 7) (27, 5) \n",
        "sleep (5, 9) (6, 5) (9, 9) (10, 1) (11, 3) \n",
        "That (8, 0) (14, 0) (19, 0) \n",
        "by (5, 1) (6, 3) \n",
        "a (4, 5) (6, 4) (8, 6) (21, 1) (22, 5) \n",
        "make (20, 6) (28, 3) \n",
        "die (5, 6) (9, 6) \n",
        "end (5, 3) (6, 10) \n",
        "to (1, 5) (2, 6) (4, 1) (5, 8) (6, 7) (9, 1) (9, 8) (10, 4) (27, 2) \n",
        "And (5, 0) (26, 0) (29, 0) (31, 0) (33, 0) \n",
        ", (1, 2) (1, 7) (3, 7) (4, 9) (5, 7) (6, 6) (7, 2) (9, 7) (9, 10) (10, 2) (10, 7) (10, 11) (11, 6) (11, 11) (12, 8) (15, 10) (16, 3) (16, 8) (17, 5) (17, 9) (18, 4) (19, 7) (21, 9) (22, 8) (23, 8) (24, 3) (25, 3) (25, 7) (26, 9) (28, 8) (30, 3) (30, 10) (31, 7) (32, 7) (33, 10) (34, 5) \n",
        "be (1, 1) (1, 6) (9, 2) \n",
        "To (1, 0) (5, 5) (9, 5) (10, 0) (22, 0) \n",
        "would (15, 2) (21, 5) \n",
        "this (12, 5) (32, 1) \n",
        "death , (11, 5) (23, 7) \n",
        "No (6, 0) (25, 0) \n",
        "not (1, 4) (27, 7) \n",
        ". (9, 4) (13, 4) (27, 9) (33, 6) (34, 3) \n",
        "us (13, 2) (26, 2) (28, 6) \n",
        "The (3, 0) (7, 0) (17, 0) (18, 0) (24, 0) (34, 0) \n",
        "For (11, 0) (15, 0) \n",
        "of (3, 4) (4, 7) (11, 4) (14, 3) (15, 8) (17, 2) (18, 2) (19, 3) (23, 4) (27, 8) (28, 5) (29, 5) (30, 8) (31, 2) (33, 4) \n",
        "death (11, 5) (23, 7) \n",
        "bear (15, 3) (21, 8) (26, 4) \n",
        "When (12, 0) (20, 0) \n",
        "and (3, 2) (6, 2) (7, 3) (15, 6) (18, 5) (22, 2) (31, 5) \n",
        "have (12, 2) (26, 8) \n",
        "is (1, 9) (8, 2) \n",
        "in (2, 3) (11, 1) (34, 6) \n",
        "the (1, 10) (2, 4) (7, 4) (10, 9) (13, 6) (15, 4) (16, 4) (17, 6) (18, 6) (19, 4) (23, 2) (25, 5) (29, 2) (30, 5) (33, 2) \n",
        "makes (14, 1) (26, 1) \n",
        "that (1, 8) (11, 2) (23, 1) (27, 4) \n",
        "life (14, 6) (22, 7) \n"
       ]
      }
     ],
     "prompt_number": 23
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "One interesting feature we can get from indexed models, is coverage information. This shows how many of the tokens in the original corpus data are covered by a particular pattern:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "querypattern = classencoder.buildpattern(\"and the\")\n",
      "\n",
      "print(model.coverage(querypattern))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.012698412698412698\n"
       ]
      }
     ],
     "prompt_number": 24
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Some numbers on the original corpus data can be obtained from the model:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print(\"Total amount of tokens in the corpus data:\" , model.tokens() )\n",
      "print(\"Total amount of word types in the corpus data:\" , model.types() )\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Total amount of tokens in the corpus data: 315\n",
        "Total amount of word types in the corpus data: 180\n"
       ]
      }
     ],
     "prompt_number": 25
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 25
    }
   ],
   "metadata": {}
  }
 ]
}